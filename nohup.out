pybullet build time: Oct 10 2021 16:44:59
pybullet build time: Oct 10 2021 16:44:59
pybullet build time: Oct 10 2021 16:44:59
pybullet build time: Oct 10 2021 16:44:59
pybullet build time: Oct 10 2021 16:44:59
pybullet build time: Oct 10 2021 16:44:59
pybullet build time: Oct 10 2021 16:44:59
pybullet build time: Oct 10 2021 16:44:59
pybullet build time: Oct 10 2021 16:44:59
pybullet build time: Oct 10 2021 16:44:59
pybullet build time: Oct 10 2021 16:44:59
pybullet build time: Oct 10 2021 16:44:59
pybullet build time: Oct 10 2021 16:44:59
Using cuda device
Num timesteps: 1200
Best mean reward: -inf - Last mean reward per episode: -0.09
Saving new best model to models/train_stack2/best_model
Num timesteps: 2400
Best mean reward: -0.09 - Last mean reward per episode: -0.09
Saving new best model to models/train_stack2/best_model
Num timesteps: 3600
Best mean reward: -0.09 - Last mean reward per episode: -0.08
Saving new best model to models/train_stack2/best_model
Num timesteps: 4800
Best mean reward: -0.08 - Last mean reward per episode: -0.09
Num timesteps: 6000
Best mean reward: -0.08 - Last mean reward per episode: -0.08
Num timesteps: 7200
Best mean reward: -0.08 - Last mean reward per episode: -0.09
Num timesteps: 8400
Best mean reward: -0.08 - Last mean reward per episode: -0.09
Num timesteps: 9600
Best mean reward: -0.08 - Last mean reward per episode: -0.08
Saving new best model to models/train_stack2/best_model
Num timesteps: 10800
Best mean reward: -0.08 - Last mean reward per episode: -0.08
Num timesteps: 12000
Best mean reward: -0.08 - Last mean reward per episode: -0.09
Num timesteps: 13200
Best mean reward: -0.08 - Last mean reward per episode: -0.07
Saving new best model to models/train_stack2/best_model
Num timesteps: 14400
Best mean reward: -0.07 - Last mean reward per episode: -0.06
Saving new best model to models/train_stack2/best_model
Num timesteps: 15600
Best mean reward: -0.06 - Last mean reward per episode: -0.09
Num timesteps: 16800
Best mean reward: -0.06 - Last mean reward per episode: -0.09
Num timesteps: 18000
Best mean reward: -0.06 - Last mean reward per episode: -0.05
Saving new best model to models/train_stack2/best_model
Num timesteps: 19200
Best mean reward: -0.05 - Last mean reward per episode: -0.07
Num timesteps: 20400
Best mean reward: -0.05 - Last mean reward per episode: -0.07
Num timesteps: 21600
Best mean reward: -0.05 - Last mean reward per episode: -0.09
Num timesteps: 22800
Best mean reward: -0.05 - Last mean reward per episode: -0.09
Num timesteps: 24000
Best mean reward: -0.05 - Last mean reward per episode: -0.09
-------------------------------------
| rollout/           |              |
|    ep_len_mean     | 2.76         |
|    ep_rew_mean     | -0.076000005 |
| time/              |              |
|    fps             | 3            |
|    iterations      | 1            |
|    time_elapsed    | 7807         |
|    total_timesteps | 24576        |
-------------------------------------
Num timesteps: 25200
Best mean reward: -0.05 - Last mean reward per episode: -0.06
Num timesteps: 26400
Best mean reward: -0.05 - Last mean reward per episode: -0.08
Num timesteps: 27600
Best mean reward: -0.05 - Last mean reward per episode: -0.09
Num timesteps: 28800
Best mean reward: -0.05 - Last mean reward per episode: -0.07
Num timesteps: 30000
Best mean reward: -0.05 - Last mean reward per episode: -0.08
Num timesteps: 31200
Best mean reward: -0.05 - Last mean reward per episode: -0.07
Num timesteps: 32400
Best mean reward: -0.05 - Last mean reward per episode: -0.08
Num timesteps: 33600
Best mean reward: -0.05 - Last mean reward per episode: -0.08
Num timesteps: 34800
Best mean reward: -0.05 - Last mean reward per episode: -0.09
Num timesteps: 36000
Best mean reward: -0.05 - Last mean reward per episode: -0.07
Num timesteps: 37200
Best mean reward: -0.05 - Last mean reward per episode: -0.07
Num timesteps: 38400
Best mean reward: -0.05 - Last mean reward per episode: -0.06
Num timesteps: 39600
Best mean reward: -0.05 - Last mean reward per episode: -0.09
Num timesteps: 40800
Best mean reward: -0.05 - Last mean reward per episode: -0.07
Num timesteps: 42000
Best mean reward: -0.05 - Last mean reward per episode: -0.09
Num timesteps: 43200
Best mean reward: -0.05 - Last mean reward per episode: -0.09
Num timesteps: 44400
Best mean reward: -0.05 - Last mean reward per episode: -0.09
Num timesteps: 45600
Best mean reward: -0.05 - Last mean reward per episode: -0.07
Num timesteps: 46800
Best mean reward: -0.05 - Last mean reward per episode: -0.08
Num timesteps: 48000
Best mean reward: -0.05 - Last mean reward per episode: -0.07
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.8         |
|    ep_rew_mean          | -0.062      |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 2           |
|    time_elapsed         | 15644       |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.015511528 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | -0.135      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0128     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0146     |
|    std                  | 0.984       |
|    value_loss           | 0.0136      |
-----------------------------------------
Num timesteps: 49200
Best mean reward: -0.05 - Last mean reward per episode: -0.06
Num timesteps: 50400
Best mean reward: -0.05 - Last mean reward per episode: -0.09
Num timesteps: 51600
Best mean reward: -0.05 - Last mean reward per episode: -0.06
Num timesteps: 52800
Best mean reward: -0.05 - Last mean reward per episode: -0.08
Num timesteps: 54000
Best mean reward: -0.05 - Last mean reward per episode: -0.08
Num timesteps: 55200
Best mean reward: -0.05 - Last mean reward per episode: -0.07
Num timesteps: 56400
Best mean reward: -0.05 - Last mean reward per episode: -0.08
Num timesteps: 57600
Best mean reward: -0.05 - Last mean reward per episode: -0.05
Saving new best model to models/train_stack2/best_model
Num timesteps: 58800
Best mean reward: -0.05 - Last mean reward per episode: -0.09
Num timesteps: 60000
Best mean reward: -0.05 - Last mean reward per episode: -0.07
Num timesteps: 61200
Best mean reward: -0.05 - Last mean reward per episode: -0.06
Num timesteps: 62400
Best mean reward: -0.05 - Last mean reward per episode: -0.08
Num timesteps: 63600
Best mean reward: -0.05 - Last mean reward per episode: -0.06
Num timesteps: 64800
Best mean reward: -0.05 - Last mean reward per episode: -0.08
Num timesteps: 66000
Best mean reward: -0.05 - Last mean reward per episode: -0.08
Num timesteps: 67200
Best mean reward: -0.05 - Last mean reward per episode: -0.09
Num timesteps: 68400
Best mean reward: -0.05 - Last mean reward per episode: -0.06
Num timesteps: 69600
Best mean reward: -0.05 - Last mean reward per episode: -0.08
Num timesteps: 70800
Best mean reward: -0.05 - Last mean reward per episode: -0.05
Saving new best model to models/train_stack2/best_model
Num timesteps: 72000
Best mean reward: -0.05 - Last mean reward per episode: -0.04
Saving new best model to models/train_stack2/best_model
Num timesteps: 73200
Best mean reward: -0.04 - Last mean reward per episode: -0.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.37        |
|    ep_rew_mean          | -0.07700001 |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 3           |
|    time_elapsed         | 23541       |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.019877588 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | -0.00422    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.013      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0164     |
|    std                  | 0.951       |
|    value_loss           | 0.0195      |
-----------------------------------------
Num timesteps: 74400
Best mean reward: -0.04 - Last mean reward per episode: -0.08
Num timesteps: 75600
Best mean reward: -0.04 - Last mean reward per episode: -0.03
Saving new best model to models/train_stack2/best_model
Num timesteps: 76800
Best mean reward: -0.03 - Last mean reward per episode: -0.08
Num timesteps: 78000
Best mean reward: -0.03 - Last mean reward per episode: -0.08
Num timesteps: 79200
Best mean reward: -0.03 - Last mean reward per episode: -0.04
Num timesteps: 80400
Best mean reward: -0.03 - Last mean reward per episode: -0.09
Num timesteps: 81600
Best mean reward: -0.03 - Last mean reward per episode: -0.08
Num timesteps: 82800
Best mean reward: -0.03 - Last mean reward per episode: -0.07
Num timesteps: 84000
Best mean reward: -0.03 - Last mean reward per episode: -0.07
Num timesteps: 85200
Best mean reward: -0.03 - Last mean reward per episode: -0.05
Num timesteps: 86400
Best mean reward: -0.03 - Last mean reward per episode: -0.05
Num timesteps: 87600
Best mean reward: -0.03 - Last mean reward per episode: -0.09
Num timesteps: 88800
Best mean reward: -0.03 - Last mean reward per episode: -0.01
Saving new best model to models/train_stack2/best_model
Num timesteps: 90000
Best mean reward: -0.01 - Last mean reward per episode: -0.04
Num timesteps: 91200
Best mean reward: -0.01 - Last mean reward per episode: -0.03
Num timesteps: 92400
Best mean reward: -0.01 - Last mean reward per episode: -0.08
Num timesteps: 93600
Best mean reward: -0.01 - Last mean reward per episode: -0.06
Num timesteps: 94800
Best mean reward: -0.01 - Last mean reward per episode: -0.05
Num timesteps: 96000
Best mean reward: -0.01 - Last mean reward per episode: -0.06
Num timesteps: 97200
Best mean reward: -0.01 - Last mean reward per episode: -0.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.74         |
|    ep_rew_mean          | -0.054000005 |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 4            |
|    time_elapsed         | 31437        |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.019113546  |
|    clip_fraction        | 0.225        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.04        |
|    explained_variance   | -0.0103      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.018       |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0198      |
|    std                  | 0.92         |
|    value_loss           | 0.0202       |
------------------------------------------
Num timesteps: 98400
Best mean reward: -0.01 - Last mean reward per episode: -0.06
Num timesteps: 99600
Best mean reward: -0.01 - Last mean reward per episode: -0.06
Num timesteps: 100800
Best mean reward: -0.01 - Last mean reward per episode: -0.05
Num timesteps: 102000
Best mean reward: -0.01 - Last mean reward per episode: -0.03
Num timesteps: 103200
Best mean reward: -0.01 - Last mean reward per episode: -0.08
Num timesteps: 104400
Best mean reward: -0.01 - Last mean reward per episode: -0.05
Num timesteps: 105600
Best mean reward: -0.01 - Last mean reward per episode: -0.06
Num timesteps: 106800
Best mean reward: -0.01 - Last mean reward per episode: -0.04
Num timesteps: 108000
Best mean reward: -0.01 - Last mean reward per episode: -0.03
Num timesteps: 109200
Best mean reward: -0.01 - Last mean reward per episode: -0.06
Num timesteps: 110400
Best mean reward: -0.01 - Last mean reward per episode: -0.06
Num timesteps: 111600
Best mean reward: -0.01 - Last mean reward per episode: -0.03
Num timesteps: 112800
Best mean reward: -0.01 - Last mean reward per episode: -0.04
Num timesteps: 114000
Best mean reward: -0.01 - Last mean reward per episode: -0.07
Num timesteps: 115200
Best mean reward: -0.01 - Last mean reward per episode: -0.05
Num timesteps: 116400
Best mean reward: -0.01 - Last mean reward per episode: -0.05
Num timesteps: 117600
Best mean reward: -0.01 - Last mean reward per episode: -0.00
Saving new best model to models/train_stack2/best_model
Num timesteps: 118800
Best mean reward: -0.00 - Last mean reward per episode: -0.07
Num timesteps: 120000
Best mean reward: -0.00 - Last mean reward per episode: -0.01
Num timesteps: 121200
Best mean reward: -0.00 - Last mean reward per episode: -0.08
Num timesteps: 122400
Best mean reward: -0.00 - Last mean reward per episode: -0.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.72        |
|    ep_rew_mean          | -0.064      |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 5           |
|    time_elapsed         | 39364       |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.021890907 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | -0.011      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0248     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0245     |
|    std                  | 0.889       |
|    value_loss           | 0.0255      |
-----------------------------------------
Num timesteps: 123600
Best mean reward: -0.00 - Last mean reward per episode: -0.04
Num timesteps: 124800
Best mean reward: -0.00 - Last mean reward per episode: -0.05
Num timesteps: 126000
Best mean reward: -0.00 - Last mean reward per episode: -0.01
Num timesteps: 127200
Best mean reward: -0.00 - Last mean reward per episode: -0.04
Num timesteps: 128400
Best mean reward: -0.00 - Last mean reward per episode: -0.05
Num timesteps: 129600
Best mean reward: -0.00 - Last mean reward per episode: -0.04
Num timesteps: 130800
Best mean reward: -0.00 - Last mean reward per episode: -0.05
Num timesteps: 132000
Best mean reward: -0.00 - Last mean reward per episode: -0.06
Num timesteps: 133200
Best mean reward: -0.00 - Last mean reward per episode: -0.02
Num timesteps: 134400
Best mean reward: -0.00 - Last mean reward per episode: -0.03
Num timesteps: 135600
Best mean reward: -0.00 - Last mean reward per episode: -0.05
Num timesteps: 136800
Best mean reward: -0.00 - Last mean reward per episode: -0.01
Num timesteps: 138000
Best mean reward: -0.00 - Last mean reward per episode: -0.05
Num timesteps: 139200
Best mean reward: -0.00 - Last mean reward per episode: -0.04
Num timesteps: 140400
Best mean reward: -0.00 - Last mean reward per episode: -0.05
Num timesteps: 141600
Best mean reward: -0.00 - Last mean reward per episode: -0.02
Num timesteps: 142800
Best mean reward: -0.00 - Last mean reward per episode: -0.03
Num timesteps: 144000
Best mean reward: -0.00 - Last mean reward per episode: -0.03
Num timesteps: 145200
Best mean reward: -0.00 - Last mean reward per episode: 0.01
Saving new best model to models/train_stack2/best_model
Num timesteps: 146400
Best mean reward: 0.01 - Last mean reward per episode: -0.03
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.25      |
|    ep_rew_mean          | -0.064    |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 6         |
|    time_elapsed         | 47290     |
|    total_timesteps      | 147456    |
| train/                  |           |
|    approx_kl            | 0.0248305 |
|    clip_fraction        | 0.254     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.84     |
|    explained_variance   | -0.0044   |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0223   |
|    n_updates            | 50        |
|    policy_gradient_loss | -0.0296   |
|    std                  | 0.862     |
|    value_loss           | 0.032     |
---------------------------------------
Num timesteps: 147600
Best mean reward: 0.01 - Last mean reward per episode: -0.06
Num timesteps: 148800
Best mean reward: 0.01 - Last mean reward per episode: 0.01
Num timesteps: 150000
Best mean reward: 0.01 - Last mean reward per episode: -0.07
Num timesteps: 151200
Best mean reward: 0.01 - Last mean reward per episode: -0.05
Num timesteps: 152400
Best mean reward: 0.01 - Last mean reward per episode: -0.00
Num timesteps: 153600
Best mean reward: 0.01 - Last mean reward per episode: 0.00
Num timesteps: 154800
Best mean reward: 0.01 - Last mean reward per episode: 0.00
Num timesteps: 156000
Best mean reward: 0.01 - Last mean reward per episode: -0.05
Num timesteps: 157200
Best mean reward: 0.01 - Last mean reward per episode: -0.04
Num timesteps: 158400
Best mean reward: 0.01 - Last mean reward per episode: -0.03
Num timesteps: 159600
Best mean reward: 0.01 - Last mean reward per episode: -0.03
Num timesteps: 160800
Best mean reward: 0.01 - Last mean reward per episode: 0.00
Num timesteps: 162000
Best mean reward: 0.01 - Last mean reward per episode: -0.00
Num timesteps: 163200
Best mean reward: 0.01 - Last mean reward per episode: -0.04
Num timesteps: 164400
Best mean reward: 0.01 - Last mean reward per episode: -0.04
Num timesteps: 165600
Best mean reward: 0.01 - Last mean reward per episode: 0.01
Num timesteps: 166800
Best mean reward: 0.01 - Last mean reward per episode: -0.03
Num timesteps: 168000
Best mean reward: 0.01 - Last mean reward per episode: -0.02
Num timesteps: 169200
Best mean reward: 0.01 - Last mean reward per episode: -0.00
Num timesteps: 170400
Best mean reward: 0.01 - Last mean reward per episode: -0.03
Num timesteps: 171600
Best mean reward: 0.01 - Last mean reward per episode: -0.02
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.48         |
|    ep_rew_mean          | -0.057000004 |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 7            |
|    time_elapsed         | 55199        |
|    total_timesteps      | 172032       |
| train/                  |              |
|    approx_kl            | 0.031316373  |
|    clip_fraction        | 0.298        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.73        |
|    explained_variance   | -6.77e-05    |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0389      |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0369      |
|    std                  | 0.833        |
|    value_loss           | 0.0403       |
------------------------------------------
Num timesteps: 172800
Best mean reward: 0.01 - Last mean reward per episode: -0.05
Num timesteps: 174000
Best mean reward: 0.01 - Last mean reward per episode: -0.02
Num timesteps: 175200
Best mean reward: 0.01 - Last mean reward per episode: -0.02
Num timesteps: 176400
Best mean reward: 0.01 - Last mean reward per episode: -0.02
Num timesteps: 177600
Best mean reward: 0.01 - Last mean reward per episode: -0.02
Num timesteps: 178800
Best mean reward: 0.01 - Last mean reward per episode: 0.01
Saving new best model to models/train_stack2/best_model
Num timesteps: 180000
Best mean reward: 0.01 - Last mean reward per episode: 0.02
Saving new best model to models/train_stack2/best_model
Num timesteps: 181200
Best mean reward: 0.02 - Last mean reward per episode: -0.03
Num timesteps: 182400
Best mean reward: 0.02 - Last mean reward per episode: -0.01
Num timesteps: 183600
Best mean reward: 0.02 - Last mean reward per episode: 0.04
Saving new best model to models/train_stack2/best_model
Num timesteps: 184800
Best mean reward: 0.04 - Last mean reward per episode: -0.03
Num timesteps: 186000
Best mean reward: 0.04 - Last mean reward per episode: -0.01
Num timesteps: 187200
Best mean reward: 0.04 - Last mean reward per episode: -0.02
Num timesteps: 188400
Best mean reward: 0.04 - Last mean reward per episode: -0.05
Num timesteps: 189600
Best mean reward: 0.04 - Last mean reward per episode: -0.00
Num timesteps: 190800
Best mean reward: 0.04 - Last mean reward per episode: -0.04
Num timesteps: 192000
Best mean reward: 0.04 - Last mean reward per episode: -0.02
Num timesteps: 193200
Best mean reward: 0.04 - Last mean reward per episode: 0.01
Num timesteps: 194400
Best mean reward: 0.04 - Last mean reward per episode: -0.03
Num timesteps: 195600
Best mean reward: 0.04 - Last mean reward per episode: 0.04
Saving new best model to models/train_stack2/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.27        |
|    ep_rew_mean          | 0.009999993 |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 8           |
|    time_elapsed         | 63136       |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.040082876 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.00247     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0368     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0418     |
|    std                  | 0.809       |
|    value_loss           | 0.0461      |
-----------------------------------------
Num timesteps: 196800
Best mean reward: 0.04 - Last mean reward per episode: 0.01
Num timesteps: 198000
Best mean reward: 0.04 - Last mean reward per episode: 0.07
Saving new best model to models/train_stack2/best_model
Num timesteps: 199200
Best mean reward: 0.07 - Last mean reward per episode: 0.04
Num timesteps: 200400
Best mean reward: 0.07 - Last mean reward per episode: -0.04
Num timesteps: 201600
Best mean reward: 0.07 - Last mean reward per episode: -0.00
Num timesteps: 202800
Best mean reward: 0.07 - Last mean reward per episode: -0.00
Num timesteps: 204000
Best mean reward: 0.07 - Last mean reward per episode: -0.00
Num timesteps: 205200
Best mean reward: 0.07 - Last mean reward per episode: -0.02
Num timesteps: 206400
Best mean reward: 0.07 - Last mean reward per episode: 0.02
Num timesteps: 207600
Best mean reward: 0.07 - Last mean reward per episode: -0.02
Num timesteps: 208800
Best mean reward: 0.07 - Last mean reward per episode: -0.01
Num timesteps: 210000
Best mean reward: 0.07 - Last mean reward per episode: 0.05
Num timesteps: 211200
Best mean reward: 0.07 - Last mean reward per episode: -0.01
Num timesteps: 212400
Best mean reward: 0.07 - Last mean reward per episode: 0.02
Num timesteps: 213600
Best mean reward: 0.07 - Last mean reward per episode: 0.04
Num timesteps: 214800
Best mean reward: 0.07 - Last mean reward per episode: -0.02
Num timesteps: 216000
Best mean reward: 0.07 - Last mean reward per episode: 0.03
Num timesteps: 217200
Best mean reward: 0.07 - Last mean reward per episode: -0.01
Num timesteps: 218400
Best mean reward: 0.07 - Last mean reward per episode: 0.02
Num timesteps: 219600
Best mean reward: 0.07 - Last mean reward per episode: 0.01
Num timesteps: 220800
Best mean reward: 0.07 - Last mean reward per episode: -0.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.29        |
|    ep_rew_mean          | 0.000999996 |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 9           |
|    time_elapsed         | 71075       |
|    total_timesteps      | 221184      |
| train/                  |             |
|    approx_kl            | 0.049690317 |
|    clip_fraction        | 0.366       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.55       |
|    explained_variance   | -0.00456    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0441     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0476     |
|    std                  | 0.788       |
|    value_loss           | 0.0529      |
-----------------------------------------
Num timesteps: 222000
Best mean reward: 0.07 - Last mean reward per episode: 0.03
Num timesteps: 223200
Best mean reward: 0.07 - Last mean reward per episode: 0.03
Num timesteps: 224400
Best mean reward: 0.07 - Last mean reward per episode: 0.00
Num timesteps: 225600
Best mean reward: 0.07 - Last mean reward per episode: 0.02
Num timesteps: 226800
Best mean reward: 0.07 - Last mean reward per episode: 0.03
Num timesteps: 228000
Best mean reward: 0.07 - Last mean reward per episode: 0.01
Num timesteps: 229200
Best mean reward: 0.07 - Last mean reward per episode: -0.00
Num timesteps: 230400
Best mean reward: 0.07 - Last mean reward per episode: 0.00
Num timesteps: 231600
Best mean reward: 0.07 - Last mean reward per episode: 0.01
Num timesteps: 232800
Best mean reward: 0.07 - Last mean reward per episode: -0.03
Num timesteps: 234000
Best mean reward: 0.07 - Last mean reward per episode: 0.06
Num timesteps: 235200
Best mean reward: 0.07 - Last mean reward per episode: 0.06
Num timesteps: 236400
Best mean reward: 0.07 - Last mean reward per episode: 0.06
Num timesteps: 237600
Best mean reward: 0.07 - Last mean reward per episode: 0.02
Num timesteps: 238800
Best mean reward: 0.07 - Last mean reward per episode: 0.09
Saving new best model to models/train_stack2/best_model
Num timesteps: 240000
Best mean reward: 0.09 - Last mean reward per episode: 0.07
Num timesteps: 241200
Best mean reward: 0.09 - Last mean reward per episode: 0.01
Num timesteps: 242400
Best mean reward: 0.09 - Last mean reward per episode: 0.05
Num timesteps: 243600
Best mean reward: 0.09 - Last mean reward per episode: -0.01
Num timesteps: 244800
Best mean reward: 0.09 - Last mean reward per episode: -0.01
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 4.74          |
|    ep_rew_mean          | -0.0020000024 |
| time/                   |               |
|    fps                  | 3             |
|    iterations           | 10            |
|    time_elapsed         | 78979         |
|    total_timesteps      | 245760        |
| train/                  |               |
|    approx_kl            | 0.064023815   |
|    clip_fraction        | 0.397         |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.47         |
|    explained_variance   | 0.00252       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0505       |
|    n_updates            | 90            |
|    policy_gradient_loss | -0.0544       |
|    std                  | 0.767         |
|    value_loss           | 0.0643        |
-------------------------------------------
Num timesteps: 246000
Best mean reward: 0.09 - Last mean reward per episode: 0.05
Num timesteps: 247200
Best mean reward: 0.09 - Last mean reward per episode: 0.07
Num timesteps: 248400
Best mean reward: 0.09 - Last mean reward per episode: 0.02
Num timesteps: 249600
Best mean reward: 0.09 - Last mean reward per episode: 0.04
Num timesteps: 250800
Best mean reward: 0.09 - Last mean reward per episode: 0.04
Num timesteps: 252000
Best mean reward: 0.09 - Last mean reward per episode: 0.05
Num timesteps: 253200
Best mean reward: 0.09 - Last mean reward per episode: -0.00
Num timesteps: 254400
Best mean reward: 0.09 - Last mean reward per episode: 0.04
Num timesteps: 255600
Best mean reward: 0.09 - Last mean reward per episode: 0.01
Num timesteps: 256800
Best mean reward: 0.09 - Last mean reward per episode: 0.02
Num timesteps: 258000
Best mean reward: 0.09 - Last mean reward per episode: 0.05
Num timesteps: 259200
Best mean reward: 0.09 - Last mean reward per episode: 0.04
Num timesteps: 260400
Best mean reward: 0.09 - Last mean reward per episode: 0.08
Num timesteps: 261600
Best mean reward: 0.09 - Last mean reward per episode: -0.01
Num timesteps: 262800
Best mean reward: 0.09 - Last mean reward per episode: 0.12
Saving new best model to models/train_stack2/best_model
Num timesteps: 264000
Best mean reward: 0.12 - Last mean reward per episode: 0.10
Num timesteps: 265200
Best mean reward: 0.12 - Last mean reward per episode: 0.02
Num timesteps: 266400
Best mean reward: 0.12 - Last mean reward per episode: 0.05
Num timesteps: 267600
Best mean reward: 0.12 - Last mean reward per episode: 0.02
Num timesteps: 268800
Best mean reward: 0.12 - Last mean reward per episode: 0.00
Num timesteps: 270000
Best mean reward: 0.12 - Last mean reward per episode: 0.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.15        |
|    ep_rew_mean          | 0.023999998 |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 11          |
|    time_elapsed         | 86864       |
|    total_timesteps      | 270336      |
| train/                  |             |
|    approx_kl            | 0.076986805 |
|    clip_fraction        | 0.427       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | -0.00318    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0536     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0585     |
|    std                  | 0.747       |
|    value_loss           | 0.0672      |
-----------------------------------------
Num timesteps: 271200
Best mean reward: 0.12 - Last mean reward per episode: 0.06
Num timesteps: 272400
Best mean reward: 0.12 - Last mean reward per episode: 0.06
Num timesteps: 273600
Best mean reward: 0.12 - Last mean reward per episode: 0.09
Num timesteps: 274800
Best mean reward: 0.12 - Last mean reward per episode: 0.02
Num timesteps: 276000
Best mean reward: 0.12 - Last mean reward per episode: 0.05
Num timesteps: 277200
Best mean reward: 0.12 - Last mean reward per episode: -0.01
Num timesteps: 278400
Best mean reward: 0.12 - Last mean reward per episode: 0.05
Num timesteps: 279600
Best mean reward: 0.12 - Last mean reward per episode: 0.04
Num timesteps: 280800
Best mean reward: 0.12 - Last mean reward per episode: 0.03
Num timesteps: 282000
Best mean reward: 0.12 - Last mean reward per episode: 0.10
Num timesteps: 283200
Best mean reward: 0.12 - Last mean reward per episode: 0.11
Num timesteps: 284400
Best mean reward: 0.12 - Last mean reward per episode: 0.13
Saving new best model to models/train_stack2/best_model
Num timesteps: 285600
Best mean reward: 0.13 - Last mean reward per episode: 0.08
Num timesteps: 286800
Best mean reward: 0.13 - Last mean reward per episode: 0.12
Num timesteps: 288000
Best mean reward: 0.13 - Last mean reward per episode: 0.12
Num timesteps: 289200
Best mean reward: 0.13 - Last mean reward per episode: 0.03
Num timesteps: 290400
Best mean reward: 0.13 - Last mean reward per episode: 0.09
Num timesteps: 291600
Best mean reward: 0.13 - Last mean reward per episode: 0.04
Num timesteps: 292800
Best mean reward: 0.13 - Last mean reward per episode: 0.06
Num timesteps: 294000
Best mean reward: 0.13 - Last mean reward per episode: 0.02
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.37       |
|    ep_rew_mean          | 0.15699999 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 12         |
|    time_elapsed         | 94740      |
|    total_timesteps      | 294912     |
| train/                  |            |
|    approx_kl            | 0.08942636 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.31      |
|    explained_variance   | 0.0125     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0555    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0621    |
|    std                  | 0.726      |
|    value_loss           | 0.081      |
----------------------------------------
Num timesteps: 295200
Best mean reward: 0.13 - Last mean reward per episode: 0.09
Num timesteps: 296400
Best mean reward: 0.13 - Last mean reward per episode: -0.01
Num timesteps: 297600
Best mean reward: 0.13 - Last mean reward per episode: 0.08
Num timesteps: 298800
Best mean reward: 0.13 - Last mean reward per episode: 0.01
Num timesteps: 300000
Best mean reward: 0.13 - Last mean reward per episode: 0.11
Num timesteps: 301200
Best mean reward: 0.13 - Last mean reward per episode: 0.06
Num timesteps: 302400
Best mean reward: 0.13 - Last mean reward per episode: 0.06
Num timesteps: 303600
Best mean reward: 0.13 - Last mean reward per episode: 0.04
Num timesteps: 304800
Best mean reward: 0.13 - Last mean reward per episode: 0.09
Num timesteps: 306000
Best mean reward: 0.13 - Last mean reward per episode: 0.11
Num timesteps: 307200
Best mean reward: 0.13 - Last mean reward per episode: 0.08
Num timesteps: 308400
Best mean reward: 0.13 - Last mean reward per episode: 0.10
Num timesteps: 309600
Best mean reward: 0.13 - Last mean reward per episode: 0.05
Num timesteps: 310800
Best mean reward: 0.13 - Last mean reward per episode: 0.08
Num timesteps: 312000
Best mean reward: 0.13 - Last mean reward per episode: 0.04
Num timesteps: 313200
Best mean reward: 0.13 - Last mean reward per episode: 0.09
Num timesteps: 314400
Best mean reward: 0.13 - Last mean reward per episode: 0.08
Num timesteps: 315600
Best mean reward: 0.13 - Last mean reward per episode: 0.12
Num timesteps: 316800
Best mean reward: 0.13 - Last mean reward per episode: 0.08
Num timesteps: 318000
Best mean reward: 0.13 - Last mean reward per episode: 0.00
Num timesteps: 319200
Best mean reward: 0.13 - Last mean reward per episode: 0.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.85        |
|    ep_rew_mean          | 0.11400001  |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 13          |
|    time_elapsed         | 102620      |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.105029315 |
|    clip_fraction        | 0.471       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.23       |
|    explained_variance   | 0.0257      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0519     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0684     |
|    std                  | 0.708       |
|    value_loss           | 0.0906      |
-----------------------------------------
Num timesteps: 320400
Best mean reward: 0.13 - Last mean reward per episode: 0.15
Saving new best model to models/train_stack2/best_model
Num timesteps: 321600
Best mean reward: 0.15 - Last mean reward per episode: 0.16
Saving new best model to models/train_stack2/best_model
Num timesteps: 322800
Best mean reward: 0.16 - Last mean reward per episode: 0.07
Num timesteps: 324000
Best mean reward: 0.16 - Last mean reward per episode: 0.11
Num timesteps: 325200
Best mean reward: 0.16 - Last mean reward per episode: 0.04
Num timesteps: 326400
Best mean reward: 0.16 - Last mean reward per episode: -0.00
Num timesteps: 327600
Best mean reward: 0.16 - Last mean reward per episode: 0.07
Num timesteps: 328800
Best mean reward: 0.16 - Last mean reward per episode: 0.13
Num timesteps: 330000
Best mean reward: 0.16 - Last mean reward per episode: 0.08
Num timesteps: 331200
Best mean reward: 0.16 - Last mean reward per episode: 0.07
Num timesteps: 332400
Best mean reward: 0.16 - Last mean reward per episode: 0.14
Num timesteps: 333600
Best mean reward: 0.16 - Last mean reward per episode: 0.21
Saving new best model to models/train_stack2/best_model
Num timesteps: 334800
Best mean reward: 0.21 - Last mean reward per episode: 0.09
Num timesteps: 336000
Best mean reward: 0.21 - Last mean reward per episode: 0.11
Num timesteps: 337200
Best mean reward: 0.21 - Last mean reward per episode: 0.12
Num timesteps: 338400
Best mean reward: 0.21 - Last mean reward per episode: 0.11
Num timesteps: 339600
Best mean reward: 0.21 - Last mean reward per episode: 0.09
Num timesteps: 340800
Best mean reward: 0.21 - Last mean reward per episode: 0.18
Num timesteps: 342000
Best mean reward: 0.21 - Last mean reward per episode: 0.10
Num timesteps: 343200
Best mean reward: 0.21 - Last mean reward per episode: 0.10
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.39        |
|    ep_rew_mean          | 0.055999998 |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 14          |
|    time_elapsed         | 110507      |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.12215081  |
|    clip_fraction        | 0.496       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.15       |
|    explained_variance   | 0.019       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.058      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0734     |
|    std                  | 0.687       |
|    value_loss           | 0.0985      |
-----------------------------------------
Num timesteps: 344400
Best mean reward: 0.21 - Last mean reward per episode: 0.05
Num timesteps: 345600
Best mean reward: 0.21 - Last mean reward per episode: 0.15
Num timesteps: 346800
Best mean reward: 0.21 - Last mean reward per episode: 0.10
Num timesteps: 348000
Best mean reward: 0.21 - Last mean reward per episode: 0.15
Num timesteps: 349200
Best mean reward: 0.21 - Last mean reward per episode: 0.01
Num timesteps: 350400
Best mean reward: 0.21 - Last mean reward per episode: 0.13
Num timesteps: 351600
Best mean reward: 0.21 - Last mean reward per episode: 0.09
Num timesteps: 352800
Best mean reward: 0.21 - Last mean reward per episode: 0.12
Num timesteps: 354000
Best mean reward: 0.21 - Last mean reward per episode: 0.09
Num timesteps: 355200
Best mean reward: 0.21 - Last mean reward per episode: 0.13
Num timesteps: 356400
Best mean reward: 0.21 - Last mean reward per episode: 0.12
Num timesteps: 357600
Best mean reward: 0.21 - Last mean reward per episode: 0.12
Num timesteps: 358800
Best mean reward: 0.21 - Last mean reward per episode: 0.15
Num timesteps: 360000
Best mean reward: 0.21 - Last mean reward per episode: 0.05
Num timesteps: 361200
Best mean reward: 0.21 - Last mean reward per episode: 0.10
Num timesteps: 362400
Best mean reward: 0.21 - Last mean reward per episode: 0.11
Num timesteps: 363600
Best mean reward: 0.21 - Last mean reward per episode: 0.11
Num timesteps: 364800
Best mean reward: 0.21 - Last mean reward per episode: 0.11
Num timesteps: 366000
Best mean reward: 0.21 - Last mean reward per episode: 0.07
Num timesteps: 367200
Best mean reward: 0.21 - Last mean reward per episode: 0.08
Num timesteps: 368400
Best mean reward: 0.21 - Last mean reward per episode: 0.14
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.61       |
|    ep_rew_mean          | 0.13599998 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 15         |
|    time_elapsed         | 118340     |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.13086031 |
|    clip_fraction        | 0.503      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.06      |
|    explained_variance   | 0.0263     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0572    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.076     |
|    std                  | 0.668      |
|    value_loss           | 0.107      |
----------------------------------------
Num timesteps: 369600
Best mean reward: 0.21 - Last mean reward per episode: 0.04
Num timesteps: 370800
Best mean reward: 0.21 - Last mean reward per episode: 0.13
Num timesteps: 372000
Best mean reward: 0.21 - Last mean reward per episode: 0.12
Num timesteps: 373200
Best mean reward: 0.21 - Last mean reward per episode: 0.12
Num timesteps: 374400
Best mean reward: 0.21 - Last mean reward per episode: 0.16
Num timesteps: 375600
Best mean reward: 0.21 - Last mean reward per episode: 0.11
Num timesteps: 376800
Best mean reward: 0.21 - Last mean reward per episode: 0.11
Num timesteps: 378000
Best mean reward: 0.21 - Last mean reward per episode: 0.02
Num timesteps: 379200
Best mean reward: 0.21 - Last mean reward per episode: 0.04
Num timesteps: 380400
Best mean reward: 0.21 - Last mean reward per episode: 0.20
Num timesteps: 381600
Best mean reward: 0.21 - Last mean reward per episode: 0.09
Num timesteps: 382800
Best mean reward: 0.21 - Last mean reward per episode: 0.11
Num timesteps: 384000
Best mean reward: 0.21 - Last mean reward per episode: 0.19
Num timesteps: 385200
Best mean reward: 0.21 - Last mean reward per episode: 0.12
Num timesteps: 386400
Best mean reward: 0.21 - Last mean reward per episode: 0.08
Num timesteps: 387600
Best mean reward: 0.21 - Last mean reward per episode: 0.12
Num timesteps: 388800
Best mean reward: 0.21 - Last mean reward per episode: 0.09
Num timesteps: 390000
Best mean reward: 0.21 - Last mean reward per episode: 0.12
Num timesteps: 391200
Best mean reward: 0.21 - Last mean reward per episode: 0.14
Num timesteps: 392400
Best mean reward: 0.21 - Last mean reward per episode: 0.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.29        |
|    ep_rew_mean          | 0.077999994 |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 16          |
|    time_elapsed         | 126179      |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.1475415   |
|    clip_fraction        | 0.531       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.98       |
|    explained_variance   | 0.0241      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0607     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0793     |
|    std                  | 0.648       |
|    value_loss           | 0.11        |
-----------------------------------------
Num timesteps: 393600
Best mean reward: 0.21 - Last mean reward per episode: 0.13
Num timesteps: 394800
Best mean reward: 0.21 - Last mean reward per episode: 0.24
Saving new best model to models/train_stack2/best_model
Num timesteps: 396000
Best mean reward: 0.24 - Last mean reward per episode: 0.10
Num timesteps: 397200
Best mean reward: 0.24 - Last mean reward per episode: 0.11
Num timesteps: 398400
Best mean reward: 0.24 - Last mean reward per episode: 0.04
Num timesteps: 399600
Best mean reward: 0.24 - Last mean reward per episode: 0.12
Num timesteps: 400800
Best mean reward: 0.24 - Last mean reward per episode: 0.05
Num timesteps: 402000
Best mean reward: 0.24 - Last mean reward per episode: 0.07
Num timesteps: 403200
Best mean reward: 0.24 - Last mean reward per episode: 0.11
Num timesteps: 404400
Best mean reward: 0.24 - Last mean reward per episode: 0.08
Num timesteps: 405600
Best mean reward: 0.24 - Last mean reward per episode: 0.14
Num timesteps: 406800
Best mean reward: 0.24 - Last mean reward per episode: 0.10
Num timesteps: 408000
Best mean reward: 0.24 - Last mean reward per episode: 0.10
Num timesteps: 409200
Best mean reward: 0.24 - Last mean reward per episode: 0.16
Num timesteps: 410400
Best mean reward: 0.24 - Last mean reward per episode: 0.07
Num timesteps: 411600
Best mean reward: 0.24 - Last mean reward per episode: 0.14
Num timesteps: 412800
Best mean reward: 0.24 - Last mean reward per episode: 0.13
Num timesteps: 414000
Best mean reward: 0.24 - Last mean reward per episode: 0.13
Num timesteps: 415200
Best mean reward: 0.24 - Last mean reward per episode: 0.13
Num timesteps: 416400
Best mean reward: 0.24 - Last mean reward per episode: 0.15
Num timesteps: 417600
Best mean reward: 0.24 - Last mean reward per episode: 0.04
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.68       |
|    ep_rew_mean          | 0.05       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 17         |
|    time_elapsed         | 134034     |
|    total_timesteps      | 417792     |
| train/                  |            |
|    approx_kl            | 0.16564794 |
|    clip_fraction        | 0.546      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.89      |
|    explained_variance   | 0.0195     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0721    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0836    |
|    std                  | 0.629      |
|    value_loss           | 0.118      |
----------------------------------------
Num timesteps: 418800
Best mean reward: 0.24 - Last mean reward per episode: 0.12
Num timesteps: 420000
Best mean reward: 0.24 - Last mean reward per episode: 0.11
Num timesteps: 421200
Best mean reward: 0.24 - Last mean reward per episode: 0.14
Num timesteps: 422400
Best mean reward: 0.24 - Last mean reward per episode: 0.18
Num timesteps: 423600
Best mean reward: 0.24 - Last mean reward per episode: 0.13
Num timesteps: 424800
Best mean reward: 0.24 - Last mean reward per episode: 0.12
Num timesteps: 426000
Best mean reward: 0.24 - Last mean reward per episode: 0.15
Num timesteps: 427200
Best mean reward: 0.24 - Last mean reward per episode: 0.22
Num timesteps: 428400
Best mean reward: 0.24 - Last mean reward per episode: 0.22
Num timesteps: 429600
Best mean reward: 0.24 - Last mean reward per episode: 0.06
Num timesteps: 430800
Best mean reward: 0.24 - Last mean reward per episode: 0.13
Num timesteps: 432000
Best mean reward: 0.24 - Last mean reward per episode: 0.17
Num timesteps: 433200
Best mean reward: 0.24 - Last mean reward per episode: 0.09
Num timesteps: 434400
Best mean reward: 0.24 - Last mean reward per episode: 0.17
Num timesteps: 435600
Best mean reward: 0.24 - Last mean reward per episode: 0.14
Num timesteps: 436800
Best mean reward: 0.24 - Last mean reward per episode: 0.16
Num timesteps: 438000
Best mean reward: 0.24 - Last mean reward per episode: 0.19
Num timesteps: 439200
Best mean reward: 0.24 - Last mean reward per episode: 0.10
Num timesteps: 440400
Best mean reward: 0.24 - Last mean reward per episode: 0.20
Num timesteps: 441600
Best mean reward: 0.24 - Last mean reward per episode: 0.13
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.45       |
|    ep_rew_mean          | 0.129      |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 18         |
|    time_elapsed         | 141871     |
|    total_timesteps      | 442368     |
| train/                  |            |
|    approx_kl            | 0.19079955 |
|    clip_fraction        | 0.57       |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.8       |
|    explained_variance   | 0.0204     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0686    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0841    |
|    std                  | 0.61       |
|    value_loss           | 0.117      |
----------------------------------------
Num timesteps: 442800
Best mean reward: 0.24 - Last mean reward per episode: 0.16
Num timesteps: 444000
Best mean reward: 0.24 - Last mean reward per episode: 0.14
Num timesteps: 445200
Best mean reward: 0.24 - Last mean reward per episode: 0.10
Num timesteps: 446400
Best mean reward: 0.24 - Last mean reward per episode: 0.17
Num timesteps: 447600
Best mean reward: 0.24 - Last mean reward per episode: 0.18
Num timesteps: 448800
Best mean reward: 0.24 - Last mean reward per episode: 0.16
Num timesteps: 450000
Best mean reward: 0.24 - Last mean reward per episode: 0.15
Num timesteps: 451200
Best mean reward: 0.24 - Last mean reward per episode: 0.19
Num timesteps: 452400
Best mean reward: 0.24 - Last mean reward per episode: 0.24
Num timesteps: 453600
Best mean reward: 0.24 - Last mean reward per episode: 0.15
Num timesteps: 454800
Best mean reward: 0.24 - Last mean reward per episode: 0.17
Num timesteps: 456000
Best mean reward: 0.24 - Last mean reward per episode: 0.12
Num timesteps: 457200
Best mean reward: 0.24 - Last mean reward per episode: 0.11
Num timesteps: 458400
Best mean reward: 0.24 - Last mean reward per episode: 0.14
Num timesteps: 459600
Best mean reward: 0.24 - Last mean reward per episode: 0.13
Num timesteps: 460800
Best mean reward: 0.24 - Last mean reward per episode: 0.27
Saving new best model to models/train_stack2/best_model
Num timesteps: 462000
Best mean reward: 0.27 - Last mean reward per episode: 0.11
Num timesteps: 463200
Best mean reward: 0.27 - Last mean reward per episode: 0.26
Num timesteps: 464400
Best mean reward: 0.27 - Last mean reward per episode: 0.23
Num timesteps: 465600
Best mean reward: 0.27 - Last mean reward per episode: 0.19
Num timesteps: 466800
Best mean reward: 0.27 - Last mean reward per episode: 0.21
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.71       |
|    ep_rew_mean          | 0.16399999 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 19         |
|    time_elapsed         | 149724     |
|    total_timesteps      | 466944     |
| train/                  |            |
|    approx_kl            | 0.2272182  |
|    clip_fraction        | 0.577      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.71      |
|    explained_variance   | 0.0296     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0681    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0861    |
|    std                  | 0.592      |
|    value_loss           | 0.125      |
----------------------------------------
Num timesteps: 468000
Best mean reward: 0.27 - Last mean reward per episode: 0.19
Num timesteps: 469200
Best mean reward: 0.27 - Last mean reward per episode: 0.13
Num timesteps: 470400
Best mean reward: 0.27 - Last mean reward per episode: 0.25
Num timesteps: 471600
Best mean reward: 0.27 - Last mean reward per episode: 0.15
Num timesteps: 472800
Best mean reward: 0.27 - Last mean reward per episode: 0.17
Num timesteps: 474000
Best mean reward: 0.27 - Last mean reward per episode: 0.14
Num timesteps: 475200
Best mean reward: 0.27 - Last mean reward per episode: 0.16
Num timesteps: 476400
Best mean reward: 0.27 - Last mean reward per episode: 0.16
Num timesteps: 477600
Best mean reward: 0.27 - Last mean reward per episode: 0.25
Num timesteps: 478800
Best mean reward: 0.27 - Last mean reward per episode: 0.21
Num timesteps: 480000
Best mean reward: 0.27 - Last mean reward per episode: 0.31
Saving new best model to models/train_stack2/best_model
Num timesteps: 481200
Best mean reward: 0.31 - Last mean reward per episode: 0.17
Num timesteps: 482400
Best mean reward: 0.31 - Last mean reward per episode: 0.11
Num timesteps: 483600
Best mean reward: 0.31 - Last mean reward per episode: 0.24
Num timesteps: 484800
Best mean reward: 0.31 - Last mean reward per episode: 0.18
Num timesteps: 486000
Best mean reward: 0.31 - Last mean reward per episode: 0.26
Num timesteps: 487200
Best mean reward: 0.31 - Last mean reward per episode: 0.20
Num timesteps: 488400
Best mean reward: 0.31 - Last mean reward per episode: 0.17
Num timesteps: 489600
Best mean reward: 0.31 - Last mean reward per episode: 0.17
Num timesteps: 490800
Best mean reward: 0.31 - Last mean reward per episode: 0.21
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.65       |
|    ep_rew_mean          | 0.17499998 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 20         |
|    time_elapsed         | 157591     |
|    total_timesteps      | 491520     |
| train/                  |            |
|    approx_kl            | 0.22612123 |
|    clip_fraction        | 0.601      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.61      |
|    explained_variance   | 0.0205     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0662    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0927    |
|    std                  | 0.571      |
|    value_loss           | 0.138      |
----------------------------------------
Num timesteps: 492000
Best mean reward: 0.31 - Last mean reward per episode: 0.16
Num timesteps: 493200
Best mean reward: 0.31 - Last mean reward per episode: 0.25
Num timesteps: 494400
Best mean reward: 0.31 - Last mean reward per episode: 0.15
Num timesteps: 495600
Best mean reward: 0.31 - Last mean reward per episode: 0.20
Num timesteps: 496800
Best mean reward: 0.31 - Last mean reward per episode: 0.16
Num timesteps: 498000
Best mean reward: 0.31 - Last mean reward per episode: 0.22
Num timesteps: 499200
Best mean reward: 0.31 - Last mean reward per episode: 0.19
Num timesteps: 500400
Best mean reward: 0.31 - Last mean reward per episode: 0.18
Num timesteps: 501600
Best mean reward: 0.31 - Last mean reward per episode: 0.23
Num timesteps: 502800
Best mean reward: 0.31 - Last mean reward per episode: 0.23
Num timesteps: 504000
Best mean reward: 0.31 - Last mean reward per episode: 0.25
Num timesteps: 505200
Best mean reward: 0.31 - Last mean reward per episode: 0.21
Num timesteps: 506400
Best mean reward: 0.31 - Last mean reward per episode: 0.20
Num timesteps: 507600
Best mean reward: 0.31 - Last mean reward per episode: 0.25
Num timesteps: 508800
Best mean reward: 0.31 - Last mean reward per episode: 0.17
Num timesteps: 510000
Best mean reward: 0.31 - Last mean reward per episode: 0.21
Num timesteps: 511200
Best mean reward: 0.31 - Last mean reward per episode: 0.25
Num timesteps: 512400
Best mean reward: 0.31 - Last mean reward per episode: 0.20
Num timesteps: 513600
Best mean reward: 0.31 - Last mean reward per episode: 0.18
Num timesteps: 514800
Best mean reward: 0.31 - Last mean reward per episode: 0.20
Num timesteps: 516000
Best mean reward: 0.31 - Last mean reward per episode: 0.23
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.6        |
|    ep_rew_mean          | 0.20300001 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 21         |
|    time_elapsed         | 165431     |
|    total_timesteps      | 516096     |
| train/                  |            |
|    approx_kl            | 0.25551656 |
|    clip_fraction        | 0.608      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.5       |
|    explained_variance   | 0.031      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0692    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.094     |
|    std                  | 0.551      |
|    value_loss           | 0.142      |
----------------------------------------
Num timesteps: 517200
Best mean reward: 0.31 - Last mean reward per episode: 0.23
Num timesteps: 518400
Best mean reward: 0.31 - Last mean reward per episode: 0.23
Num timesteps: 519600
Best mean reward: 0.31 - Last mean reward per episode: 0.27
Num timesteps: 520800
Best mean reward: 0.31 - Last mean reward per episode: 0.27
Num timesteps: 522000
Best mean reward: 0.31 - Last mean reward per episode: 0.27
Num timesteps: 523200
Best mean reward: 0.31 - Last mean reward per episode: 0.21
Num timesteps: 524400
Best mean reward: 0.31 - Last mean reward per episode: 0.30
Num timesteps: 525600
Best mean reward: 0.31 - Last mean reward per episode: 0.21
Num timesteps: 526800
Best mean reward: 0.31 - Last mean reward per episode: 0.22
Num timesteps: 528000
Best mean reward: 0.31 - Last mean reward per episode: 0.21
Num timesteps: 529200
Best mean reward: 0.31 - Last mean reward per episode: 0.18
Num timesteps: 530400
Best mean reward: 0.31 - Last mean reward per episode: 0.25
Num timesteps: 531600
Best mean reward: 0.31 - Last mean reward per episode: 0.20
Num timesteps: 532800
Best mean reward: 0.31 - Last mean reward per episode: 0.21
Num timesteps: 534000
Best mean reward: 0.31 - Last mean reward per episode: 0.15
Num timesteps: 535200
Best mean reward: 0.31 - Last mean reward per episode: 0.20
Num timesteps: 536400
Best mean reward: 0.31 - Last mean reward per episode: 0.31
Saving new best model to models/train_stack2/best_model
Num timesteps: 537600
Best mean reward: 0.31 - Last mean reward per episode: 0.14
Num timesteps: 538800
Best mean reward: 0.31 - Last mean reward per episode: 0.21
Num timesteps: 540000
Best mean reward: 0.31 - Last mean reward per episode: 0.18
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.09       |
|    ep_rew_mean          | 0.20400001 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 22         |
|    time_elapsed         | 173282     |
|    total_timesteps      | 540672     |
| train/                  |            |
|    approx_kl            | 0.27036878 |
|    clip_fraction        | 0.623      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.4       |
|    explained_variance   | 0.0363     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0729    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0952    |
|    std                  | 0.532      |
|    value_loss           | 0.144      |
----------------------------------------
Num timesteps: 541200
Best mean reward: 0.31 - Last mean reward per episode: 0.24
Num timesteps: 542400
Best mean reward: 0.31 - Last mean reward per episode: 0.29
Num timesteps: 543600
Best mean reward: 0.31 - Last mean reward per episode: 0.10
Num timesteps: 544800
Best mean reward: 0.31 - Last mean reward per episode: 0.24
Num timesteps: 546000
Best mean reward: 0.31 - Last mean reward per episode: 0.23
Num timesteps: 547200
Best mean reward: 0.31 - Last mean reward per episode: 0.25
Num timesteps: 548400
Best mean reward: 0.31 - Last mean reward per episode: 0.23
Num timesteps: 549600
Best mean reward: 0.31 - Last mean reward per episode: 0.29
Num timesteps: 550800
Best mean reward: 0.31 - Last mean reward per episode: 0.18
Num timesteps: 552000
Best mean reward: 0.31 - Last mean reward per episode: 0.22
Num timesteps: 553200
Best mean reward: 0.31 - Last mean reward per episode: 0.20
Num timesteps: 554400
Best mean reward: 0.31 - Last mean reward per episode: 0.18
Num timesteps: 555600
Best mean reward: 0.31 - Last mean reward per episode: 0.24
Num timesteps: 556800
Best mean reward: 0.31 - Last mean reward per episode: 0.23
Num timesteps: 558000
Best mean reward: 0.31 - Last mean reward per episode: 0.27
Num timesteps: 559200
Best mean reward: 0.31 - Last mean reward per episode: 0.24
Num timesteps: 560400
Best mean reward: 0.31 - Last mean reward per episode: 0.17
Num timesteps: 561600
Best mean reward: 0.31 - Last mean reward per episode: 0.24
Num timesteps: 562800
Best mean reward: 0.31 - Last mean reward per episode: 0.16
Num timesteps: 564000
Best mean reward: 0.31 - Last mean reward per episode: 0.23
Num timesteps: 565200
Best mean reward: 0.31 - Last mean reward per episode: 0.23
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 6.31      |
|    ep_rew_mean          | 0.221     |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 23        |
|    time_elapsed         | 181122    |
|    total_timesteps      | 565248    |
| train/                  |           |
|    approx_kl            | 0.3058118 |
|    clip_fraction        | 0.636     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.31     |
|    explained_variance   | 0.0365    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0648   |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0968   |
|    std                  | 0.514     |
|    value_loss           | 0.142     |
---------------------------------------
Num timesteps: 566400
Best mean reward: 0.31 - Last mean reward per episode: 0.19
Num timesteps: 567600
Best mean reward: 0.31 - Last mean reward per episode: 0.28
Num timesteps: 568800
Best mean reward: 0.31 - Last mean reward per episode: 0.26
Num timesteps: 570000
Best mean reward: 0.31 - Last mean reward per episode: 0.32
Saving new best model to models/train_stack2/best_model
Num timesteps: 571200
Best mean reward: 0.32 - Last mean reward per episode: 0.31
Num timesteps: 572400
Best mean reward: 0.32 - Last mean reward per episode: 0.27
Num timesteps: 573600
Best mean reward: 0.32 - Last mean reward per episode: 0.17
Num timesteps: 574800
Best mean reward: 0.32 - Last mean reward per episode: 0.20
Num timesteps: 576000
Best mean reward: 0.32 - Last mean reward per episode: 0.30
Num timesteps: 577200
Best mean reward: 0.32 - Last mean reward per episode: 0.26
Num timesteps: 578400
Best mean reward: 0.32 - Last mean reward per episode: 0.26
Num timesteps: 579600
Best mean reward: 0.32 - Last mean reward per episode: 0.19
Num timesteps: 580800
Best mean reward: 0.32 - Last mean reward per episode: 0.20
Num timesteps: 582000
Best mean reward: 0.32 - Last mean reward per episode: 0.27
Num timesteps: 583200
Best mean reward: 0.32 - Last mean reward per episode: 0.25
Num timesteps: 584400
Best mean reward: 0.32 - Last mean reward per episode: 0.17
Num timesteps: 585600
Best mean reward: 0.32 - Last mean reward per episode: 0.24
Num timesteps: 586800
Best mean reward: 0.32 - Last mean reward per episode: 0.18
Num timesteps: 588000
Best mean reward: 0.32 - Last mean reward per episode: 0.27
Num timesteps: 589200
Best mean reward: 0.32 - Last mean reward per episode: 0.20
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.54       |
|    ep_rew_mean          | 0.203      |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 24         |
|    time_elapsed         | 188970     |
|    total_timesteps      | 589824     |
| train/                  |            |
|    approx_kl            | 0.32532513 |
|    clip_fraction        | 0.643      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.19      |
|    explained_variance   | 0.0422     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.074     |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0972    |
|    std                  | 0.493      |
|    value_loss           | 0.145      |
----------------------------------------
Num timesteps: 590400
Best mean reward: 0.32 - Last mean reward per episode: 0.26
Num timesteps: 591600
Best mean reward: 0.32 - Last mean reward per episode: 0.18
Num timesteps: 592800
Best mean reward: 0.32 - Last mean reward per episode: 0.30
Num timesteps: 594000
Best mean reward: 0.32 - Last mean reward per episode: 0.20
Num timesteps: 595200
Best mean reward: 0.32 - Last mean reward per episode: 0.31
Num timesteps: 596400
Best mean reward: 0.32 - Last mean reward per episode: 0.29
Num timesteps: 597600
Best mean reward: 0.32 - Last mean reward per episode: 0.32
Saving new best model to models/train_stack2/best_model
Num timesteps: 598800
Best mean reward: 0.32 - Last mean reward per episode: 0.22
Num timesteps: 600000
Best mean reward: 0.32 - Last mean reward per episode: 0.25
Num timesteps: 601200
Best mean reward: 0.32 - Last mean reward per episode: 0.27
Num timesteps: 602400
Best mean reward: 0.32 - Last mean reward per episode: 0.30
Num timesteps: 603600
Best mean reward: 0.32 - Last mean reward per episode: 0.30
Num timesteps: 604800
Best mean reward: 0.32 - Last mean reward per episode: 0.28
Num timesteps: 606000
Best mean reward: 0.32 - Last mean reward per episode: 0.28
Num timesteps: 607200
Best mean reward: 0.32 - Last mean reward per episode: 0.23
Num timesteps: 608400
Best mean reward: 0.32 - Last mean reward per episode: 0.36
Saving new best model to models/train_stack2/best_model
Num timesteps: 609600
Best mean reward: 0.36 - Last mean reward per episode: 0.29
Num timesteps: 610800
Best mean reward: 0.36 - Last mean reward per episode: 0.26
Num timesteps: 612000
Best mean reward: 0.36 - Last mean reward per episode: 0.32
Num timesteps: 613200
Best mean reward: 0.36 - Last mean reward per episode: 0.25
Num timesteps: 614400
Best mean reward: 0.36 - Last mean reward per episode: 0.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.8        |
|    ep_rew_mean          | 0.267      |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 25         |
|    time_elapsed         | 196805     |
|    total_timesteps      | 614400     |
| train/                  |            |
|    approx_kl            | 0.37892976 |
|    clip_fraction        | 0.663      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.07      |
|    explained_variance   | 0.0416     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0715    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.474      |
|    value_loss           | 0.149      |
----------------------------------------
Num timesteps: 615600
Best mean reward: 0.36 - Last mean reward per episode: 0.24
Num timesteps: 616800
Best mean reward: 0.36 - Last mean reward per episode: 0.18
Num timesteps: 618000
Best mean reward: 0.36 - Last mean reward per episode: 0.23
Num timesteps: 619200
Best mean reward: 0.36 - Last mean reward per episode: 0.24
Num timesteps: 620400
Best mean reward: 0.36 - Last mean reward per episode: 0.26
Num timesteps: 621600
Best mean reward: 0.36 - Last mean reward per episode: 0.30
Num timesteps: 622800
Best mean reward: 0.36 - Last mean reward per episode: 0.29
Num timesteps: 624000
Best mean reward: 0.36 - Last mean reward per episode: 0.39
Saving new best model to models/train_stack2/best_model
Num timesteps: 625200
Best mean reward: 0.39 - Last mean reward per episode: 0.29
Num timesteps: 626400
Best mean reward: 0.39 - Last mean reward per episode: 0.23
Num timesteps: 627600
Best mean reward: 0.39 - Last mean reward per episode: 0.30
Num timesteps: 628800
Best mean reward: 0.39 - Last mean reward per episode: 0.27
Num timesteps: 630000
Best mean reward: 0.39 - Last mean reward per episode: 0.31
Num timesteps: 631200
Best mean reward: 0.39 - Last mean reward per episode: 0.29
Num timesteps: 632400
Best mean reward: 0.39 - Last mean reward per episode: 0.32
Num timesteps: 633600
Best mean reward: 0.39 - Last mean reward per episode: 0.25
Num timesteps: 634800
Best mean reward: 0.39 - Last mean reward per episode: 0.26
Num timesteps: 636000
Best mean reward: 0.39 - Last mean reward per episode: 0.24
Num timesteps: 637200
Best mean reward: 0.39 - Last mean reward per episode: 0.25
Num timesteps: 638400
Best mean reward: 0.39 - Last mean reward per episode: 0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.57       |
|    ep_rew_mean          | 0.24099998 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 26         |
|    time_elapsed         | 205410     |
|    total_timesteps      | 638976     |
| train/                  |            |
|    approx_kl            | 0.40366197 |
|    clip_fraction        | 0.669      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.96      |
|    explained_variance   | 0.0477     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0744    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.456      |
|    value_loss           | 0.158      |
----------------------------------------
Num timesteps: 639600
Best mean reward: 0.39 - Last mean reward per episode: 0.23
Num timesteps: 640800
Best mean reward: 0.39 - Last mean reward per episode: 0.26
Num timesteps: 642000
Best mean reward: 0.39 - Last mean reward per episode: 0.23
Num timesteps: 643200
Best mean reward: 0.39 - Last mean reward per episode: 0.34
Num timesteps: 644400
Best mean reward: 0.39 - Last mean reward per episode: 0.28
Num timesteps: 645600
Best mean reward: 0.39 - Last mean reward per episode: 0.38
Num timesteps: 646800
Best mean reward: 0.39 - Last mean reward per episode: 0.30
Num timesteps: 648000
Best mean reward: 0.39 - Last mean reward per episode: 0.33
Num timesteps: 649200
Best mean reward: 0.39 - Last mean reward per episode: 0.35
Num timesteps: 650400
Best mean reward: 0.39 - Last mean reward per episode: 0.25
Num timesteps: 651600
Best mean reward: 0.39 - Last mean reward per episode: 0.30
Num timesteps: 652800
Best mean reward: 0.39 - Last mean reward per episode: 0.27
Num timesteps: 654000
Best mean reward: 0.39 - Last mean reward per episode: 0.27
Num timesteps: 655200
Best mean reward: 0.39 - Last mean reward per episode: 0.33
Num timesteps: 656400
Best mean reward: 0.39 - Last mean reward per episode: 0.25
Num timesteps: 657600
Best mean reward: 0.39 - Last mean reward per episode: 0.36
Num timesteps: 658800
Best mean reward: 0.39 - Last mean reward per episode: 0.33
Num timesteps: 660000
Best mean reward: 0.39 - Last mean reward per episode: 0.31
Num timesteps: 661200
Best mean reward: 0.39 - Last mean reward per episode: 0.31
Num timesteps: 662400
Best mean reward: 0.39 - Last mean reward per episode: 0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.51       |
|    ep_rew_mean          | 0.33500004 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 27         |
|    time_elapsed         | 213230     |
|    total_timesteps      | 663552     |
| train/                  |            |
|    approx_kl            | 0.42363098 |
|    clip_fraction        | 0.683      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.84      |
|    explained_variance   | 0.0478     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0685    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.438      |
|    value_loss           | 0.161      |
----------------------------------------
Num timesteps: 663600
Best mean reward: 0.39 - Last mean reward per episode: 0.33
Num timesteps: 664800
Best mean reward: 0.39 - Last mean reward per episode: 0.32
Num timesteps: 666000
Best mean reward: 0.39 - Last mean reward per episode: 0.43
Saving new best model to models/train_stack2/best_model
Num timesteps: 667200
Best mean reward: 0.43 - Last mean reward per episode: 0.31
Num timesteps: 668400
Best mean reward: 0.43 - Last mean reward per episode: 0.19
Num timesteps: 669600
Best mean reward: 0.43 - Last mean reward per episode: 0.28
Num timesteps: 670800
Best mean reward: 0.43 - Last mean reward per episode: 0.39
Num timesteps: 672000
Best mean reward: 0.43 - Last mean reward per episode: 0.30
Num timesteps: 673200
Best mean reward: 0.43 - Last mean reward per episode: 0.22
Num timesteps: 674400
Best mean reward: 0.43 - Last mean reward per episode: 0.33
Num timesteps: 675600
Best mean reward: 0.43 - Last mean reward per episode: 0.28
Num timesteps: 676800
Best mean reward: 0.43 - Last mean reward per episode: 0.28
Num timesteps: 678000
Best mean reward: 0.43 - Last mean reward per episode: 0.31
Num timesteps: 679200
Best mean reward: 0.43 - Last mean reward per episode: 0.28
Num timesteps: 680400
Best mean reward: 0.43 - Last mean reward per episode: 0.33
Num timesteps: 681600
Best mean reward: 0.43 - Last mean reward per episode: 0.28
Num timesteps: 682800
Best mean reward: 0.43 - Last mean reward per episode: 0.21
Num timesteps: 684000
Best mean reward: 0.43 - Last mean reward per episode: 0.34
Num timesteps: 685200
Best mean reward: 0.43 - Last mean reward per episode: 0.32
Num timesteps: 686400
Best mean reward: 0.43 - Last mean reward per episode: 0.35
Num timesteps: 687600
Best mean reward: 0.43 - Last mean reward per episode: 0.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.15       |
|    ep_rew_mean          | 0.32400006 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 28         |
|    time_elapsed         | 221061     |
|    total_timesteps      | 688128     |
| train/                  |            |
|    approx_kl            | 0.45986834 |
|    clip_fraction        | 0.692      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.72      |
|    explained_variance   | 0.0501     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0739    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.419      |
|    value_loss           | 0.165      |
----------------------------------------
Num timesteps: 688800
Best mean reward: 0.43 - Last mean reward per episode: 0.31
Num timesteps: 690000
Best mean reward: 0.43 - Last mean reward per episode: 0.34
Num timesteps: 691200
Best mean reward: 0.43 - Last mean reward per episode: 0.33
Num timesteps: 692400
Best mean reward: 0.43 - Last mean reward per episode: 0.28
Num timesteps: 693600
Best mean reward: 0.43 - Last mean reward per episode: 0.35
Num timesteps: 694800
Best mean reward: 0.43 - Last mean reward per episode: 0.30
Num timesteps: 696000
Best mean reward: 0.43 - Last mean reward per episode: 0.29
Num timesteps: 697200
Best mean reward: 0.43 - Last mean reward per episode: 0.31
Num timesteps: 698400
Best mean reward: 0.43 - Last mean reward per episode: 0.30
Num timesteps: 699600
Best mean reward: 0.43 - Last mean reward per episode: 0.21
Num timesteps: 700800
Best mean reward: 0.43 - Last mean reward per episode: 0.35
Num timesteps: 702000
Best mean reward: 0.43 - Last mean reward per episode: 0.37
Num timesteps: 703200
Best mean reward: 0.43 - Last mean reward per episode: 0.36
Num timesteps: 704400
Best mean reward: 0.43 - Last mean reward per episode: 0.37
Num timesteps: 705600
Best mean reward: 0.43 - Last mean reward per episode: 0.34
Num timesteps: 706800
Best mean reward: 0.43 - Last mean reward per episode: 0.29
Num timesteps: 708000
Best mean reward: 0.43 - Last mean reward per episode: 0.38
Num timesteps: 709200
Best mean reward: 0.43 - Last mean reward per episode: 0.34
Num timesteps: 710400
Best mean reward: 0.43 - Last mean reward per episode: 0.24
Num timesteps: 711600
Best mean reward: 0.43 - Last mean reward per episode: 0.17
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 6.36      |
|    ep_rew_mean          | 0.282     |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 29        |
|    time_elapsed         | 228866    |
|    total_timesteps      | 712704    |
| train/                  |           |
|    approx_kl            | 0.5326952 |
|    clip_fraction        | 0.699     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.58     |
|    explained_variance   | 0.0507    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0815   |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.102    |
|    std                  | 0.4       |
|    value_loss           | 0.165     |
---------------------------------------
Num timesteps: 712800
Best mean reward: 0.43 - Last mean reward per episode: 0.25
Num timesteps: 714000
Best mean reward: 0.43 - Last mean reward per episode: 0.24
Num timesteps: 715200
Best mean reward: 0.43 - Last mean reward per episode: 0.34
Num timesteps: 716400
Best mean reward: 0.43 - Last mean reward per episode: 0.30
Num timesteps: 717600
Best mean reward: 0.43 - Last mean reward per episode: 0.40
Num timesteps: 718800
Best mean reward: 0.43 - Last mean reward per episode: 0.31
Num timesteps: 720000
Best mean reward: 0.43 - Last mean reward per episode: 0.26
Num timesteps: 721200
Best mean reward: 0.43 - Last mean reward per episode: 0.33
Num timesteps: 722400
Best mean reward: 0.43 - Last mean reward per episode: 0.37
Num timesteps: 723600
Best mean reward: 0.43 - Last mean reward per episode: 0.22
Num timesteps: 724800
Best mean reward: 0.43 - Last mean reward per episode: 0.24
Num timesteps: 726000
Best mean reward: 0.43 - Last mean reward per episode: 0.30
Num timesteps: 727200
Best mean reward: 0.43 - Last mean reward per episode: 0.44
Saving new best model to models/train_stack2/best_model
Num timesteps: 728400
Best mean reward: 0.44 - Last mean reward per episode: 0.31
Num timesteps: 729600
Best mean reward: 0.44 - Last mean reward per episode: 0.28
Num timesteps: 730800
Best mean reward: 0.44 - Last mean reward per episode: 0.33
Num timesteps: 732000
Best mean reward: 0.44 - Last mean reward per episode: 0.32
Num timesteps: 733200
Best mean reward: 0.44 - Last mean reward per episode: 0.36
Num timesteps: 734400
Best mean reward: 0.44 - Last mean reward per episode: 0.27
Num timesteps: 735600
Best mean reward: 0.44 - Last mean reward per episode: 0.36
Num timesteps: 736800
Best mean reward: 0.44 - Last mean reward per episode: 0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 5.89      |
|    ep_rew_mean          | 0.357     |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 30        |
|    time_elapsed         | 236665    |
|    total_timesteps      | 737280    |
| train/                  |           |
|    approx_kl            | 0.5345169 |
|    clip_fraction        | 0.706     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.46     |
|    explained_variance   | 0.0529    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.069    |
|    n_updates            | 290       |
|    policy_gradient_loss | -0.103    |
|    std                  | 0.384     |
|    value_loss           | 0.167     |
---------------------------------------
Num timesteps: 738000
Best mean reward: 0.44 - Last mean reward per episode: 0.37
Num timesteps: 739200
Best mean reward: 0.44 - Last mean reward per episode: 0.25
Num timesteps: 740400
Best mean reward: 0.44 - Last mean reward per episode: 0.36
Num timesteps: 741600
Best mean reward: 0.44 - Last mean reward per episode: 0.34
Num timesteps: 742800
Best mean reward: 0.44 - Last mean reward per episode: 0.37
Num timesteps: 744000
Best mean reward: 0.44 - Last mean reward per episode: 0.37
Num timesteps: 745200
Best mean reward: 0.44 - Last mean reward per episode: 0.46
Saving new best model to models/train_stack2/best_model
Num timesteps: 746400
Best mean reward: 0.46 - Last mean reward per episode: 0.31
Num timesteps: 747600
Best mean reward: 0.46 - Last mean reward per episode: 0.33
Num timesteps: 748800
Best mean reward: 0.46 - Last mean reward per episode: 0.44
Num timesteps: 750000
Best mean reward: 0.46 - Last mean reward per episode: 0.28
Num timesteps: 751200
Best mean reward: 0.46 - Last mean reward per episode: 0.34
Num timesteps: 752400
Best mean reward: 0.46 - Last mean reward per episode: 0.29
Num timesteps: 753600
Best mean reward: 0.46 - Last mean reward per episode: 0.32
Num timesteps: 754800
Best mean reward: 0.46 - Last mean reward per episode: 0.35
Num timesteps: 756000
Best mean reward: 0.46 - Last mean reward per episode: 0.40
Num timesteps: 757200
Best mean reward: 0.46 - Last mean reward per episode: 0.30
Num timesteps: 758400
Best mean reward: 0.46 - Last mean reward per episode: 0.33
Num timesteps: 759600
Best mean reward: 0.46 - Last mean reward per episode: 0.32
Num timesteps: 760800
Best mean reward: 0.46 - Last mean reward per episode: 0.31
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.83       |
|    ep_rew_mean          | 0.32799998 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 31         |
|    time_elapsed         | 244480     |
|    total_timesteps      | 761856     |
| train/                  |            |
|    approx_kl            | 0.6685226  |
|    clip_fraction        | 0.719      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.0611     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0666    |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.367      |
|    value_loss           | 0.166      |
----------------------------------------
Num timesteps: 762000
Best mean reward: 0.46 - Last mean reward per episode: 0.35
Num timesteps: 763200
Best mean reward: 0.46 - Last mean reward per episode: 0.45
Num timesteps: 764400
Best mean reward: 0.46 - Last mean reward per episode: 0.25
Num timesteps: 765600
Best mean reward: 0.46 - Last mean reward per episode: 0.30
Num timesteps: 766800
Best mean reward: 0.46 - Last mean reward per episode: 0.40
Num timesteps: 768000
Best mean reward: 0.46 - Last mean reward per episode: 0.40
Num timesteps: 769200
Best mean reward: 0.46 - Last mean reward per episode: 0.34
Num timesteps: 770400
Best mean reward: 0.46 - Last mean reward per episode: 0.40
Num timesteps: 771600
Best mean reward: 0.46 - Last mean reward per episode: 0.39
Num timesteps: 772800
Best mean reward: 0.46 - Last mean reward per episode: 0.35
Num timesteps: 774000
Best mean reward: 0.46 - Last mean reward per episode: 0.45
Num timesteps: 775200
Best mean reward: 0.46 - Last mean reward per episode: 0.46
Saving new best model to models/train_stack2/best_model
Num timesteps: 776400
Best mean reward: 0.46 - Last mean reward per episode: 0.34
Num timesteps: 777600
Best mean reward: 0.46 - Last mean reward per episode: 0.45
Num timesteps: 778800
Best mean reward: 0.46 - Last mean reward per episode: 0.34
Num timesteps: 780000
Best mean reward: 0.46 - Last mean reward per episode: 0.32
Num timesteps: 781200
Best mean reward: 0.46 - Last mean reward per episode: 0.29
Num timesteps: 782400
Best mean reward: 0.46 - Last mean reward per episode: 0.33
Num timesteps: 783600
Best mean reward: 0.46 - Last mean reward per episode: 0.35
Num timesteps: 784800
Best mean reward: 0.46 - Last mean reward per episode: 0.39
Num timesteps: 786000
Best mean reward: 0.46 - Last mean reward per episode: 0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.15       |
|    ep_rew_mean          | 0.36200005 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 32         |
|    time_elapsed         | 252301     |
|    total_timesteps      | 786432     |
| train/                  |            |
|    approx_kl            | 0.62905115 |
|    clip_fraction        | 0.721      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.18      |
|    explained_variance   | 0.0645     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0744    |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.104     |
|    std                  | 0.349      |
|    value_loss           | 0.169      |
----------------------------------------
Num timesteps: 787200
Best mean reward: 0.46 - Last mean reward per episode: 0.35
Num timesteps: 788400
Best mean reward: 0.46 - Last mean reward per episode: 0.36
Num timesteps: 789600
Best mean reward: 0.46 - Last mean reward per episode: 0.49
Saving new best model to models/train_stack2/best_model
Num timesteps: 790800
Best mean reward: 0.49 - Last mean reward per episode: 0.41
Num timesteps: 792000
Best mean reward: 0.49 - Last mean reward per episode: 0.37
Num timesteps: 793200
Best mean reward: 0.49 - Last mean reward per episode: 0.34
Num timesteps: 794400
Best mean reward: 0.49 - Last mean reward per episode: 0.32
Num timesteps: 795600
Best mean reward: 0.49 - Last mean reward per episode: 0.33
Num timesteps: 796800
Best mean reward: 0.49 - Last mean reward per episode: 0.38
Num timesteps: 798000
Best mean reward: 0.49 - Last mean reward per episode: 0.37
Num timesteps: 799200
Best mean reward: 0.49 - Last mean reward per episode: 0.36
Num timesteps: 800400
Best mean reward: 0.49 - Last mean reward per episode: 0.44
Num timesteps: 801600
Best mean reward: 0.49 - Last mean reward per episode: 0.41
Num timesteps: 802800
Best mean reward: 0.49 - Last mean reward per episode: 0.39
Num timesteps: 804000
Best mean reward: 0.49 - Last mean reward per episode: 0.36
Num timesteps: 805200
Best mean reward: 0.49 - Last mean reward per episode: 0.44
Num timesteps: 806400
Best mean reward: 0.49 - Last mean reward per episode: 0.43
Num timesteps: 807600
Best mean reward: 0.49 - Last mean reward per episode: 0.33
Num timesteps: 808800
Best mean reward: 0.49 - Last mean reward per episode: 0.41
Num timesteps: 810000
Best mean reward: 0.49 - Last mean reward per episode: 0.24
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.15       |
|    ep_rew_mean          | 0.266      |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 33         |
|    time_elapsed         | 260112     |
|    total_timesteps      | 811008     |
| train/                  |            |
|    approx_kl            | 0.64536124 |
|    clip_fraction        | 0.732      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.059      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0658    |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.331      |
|    value_loss           | 0.171      |
----------------------------------------
Num timesteps: 811200
Best mean reward: 0.49 - Last mean reward per episode: 0.33
Num timesteps: 812400
Best mean reward: 0.49 - Last mean reward per episode: 0.36
Num timesteps: 813600
Best mean reward: 0.49 - Last mean reward per episode: 0.36
Num timesteps: 814800
Best mean reward: 0.49 - Last mean reward per episode: 0.41
Num timesteps: 816000
Best mean reward: 0.49 - Last mean reward per episode: 0.41
Num timesteps: 817200
Best mean reward: 0.49 - Last mean reward per episode: 0.42
Num timesteps: 818400
Best mean reward: 0.49 - Last mean reward per episode: 0.31
Num timesteps: 819600
Best mean reward: 0.49 - Last mean reward per episode: 0.36
Num timesteps: 820800
Best mean reward: 0.49 - Last mean reward per episode: 0.36
Num timesteps: 822000
Best mean reward: 0.49 - Last mean reward per episode: 0.36
Num timesteps: 823200
Best mean reward: 0.49 - Last mean reward per episode: 0.43
Num timesteps: 824400
Best mean reward: 0.49 - Last mean reward per episode: 0.35
Num timesteps: 825600
Best mean reward: 0.49 - Last mean reward per episode: 0.44
Num timesteps: 826800
Best mean reward: 0.49 - Last mean reward per episode: 0.46
Num timesteps: 828000
Best mean reward: 0.49 - Last mean reward per episode: 0.26
Num timesteps: 829200
Best mean reward: 0.49 - Last mean reward per episode: 0.36
Num timesteps: 830400
Best mean reward: 0.49 - Last mean reward per episode: 0.33
Num timesteps: 831600
Best mean reward: 0.49 - Last mean reward per episode: 0.34
Num timesteps: 832800
Best mean reward: 0.49 - Last mean reward per episode: 0.25
Num timesteps: 834000
Best mean reward: 0.49 - Last mean reward per episode: 0.38
Num timesteps: 835200
Best mean reward: 0.49 - Last mean reward per episode: 0.42
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.22       |
|    ep_rew_mean          | 0.45099998 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 34         |
|    time_elapsed         | 267911     |
|    total_timesteps      | 835584     |
| train/                  |            |
|    approx_kl            | 0.6814368  |
|    clip_fraction        | 0.736      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.878     |
|    explained_variance   | 0.0526     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0726    |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.104     |
|    std                  | 0.315      |
|    value_loss           | 0.173      |
----------------------------------------
Num timesteps: 836400
Best mean reward: 0.49 - Last mean reward per episode: 0.39
Num timesteps: 837600
Best mean reward: 0.49 - Last mean reward per episode: 0.42
Num timesteps: 838800
Best mean reward: 0.49 - Last mean reward per episode: 0.46
Num timesteps: 840000
Best mean reward: 0.49 - Last mean reward per episode: 0.46
Num timesteps: 841200
Best mean reward: 0.49 - Last mean reward per episode: 0.34
Num timesteps: 842400
Best mean reward: 0.49 - Last mean reward per episode: 0.36
Num timesteps: 843600
Best mean reward: 0.49 - Last mean reward per episode: 0.40
Num timesteps: 844800
Best mean reward: 0.49 - Last mean reward per episode: 0.38
Num timesteps: 846000
Best mean reward: 0.49 - Last mean reward per episode: 0.39
Num timesteps: 847200
Best mean reward: 0.49 - Last mean reward per episode: 0.49
Saving new best model to models/train_stack2/best_model
Num timesteps: 848400
Best mean reward: 0.49 - Last mean reward per episode: 0.34
Num timesteps: 849600
Best mean reward: 0.49 - Last mean reward per episode: 0.36
Num timesteps: 850800
Best mean reward: 0.49 - Last mean reward per episode: 0.39
Num timesteps: 852000
Best mean reward: 0.49 - Last mean reward per episode: 0.34
Num timesteps: 853200
Best mean reward: 0.49 - Last mean reward per episode: 0.48
Num timesteps: 854400
Best mean reward: 0.49 - Last mean reward per episode: 0.35
Num timesteps: 855600
Best mean reward: 0.49 - Last mean reward per episode: 0.42
Num timesteps: 856800
Best mean reward: 0.49 - Last mean reward per episode: 0.41
Num timesteps: 858000
Best mean reward: 0.49 - Last mean reward per episode: 0.30
Num timesteps: 859200
Best mean reward: 0.49 - Last mean reward per episode: 0.44
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.51       |
|    ep_rew_mean          | 0.36000004 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 35         |
|    time_elapsed         | 275683     |
|    total_timesteps      | 860160     |
| train/                  |            |
|    approx_kl            | 0.95454675 |
|    clip_fraction        | 0.748      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.725     |
|    explained_variance   | 0.0746     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0638    |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.299      |
|    value_loss           | 0.171      |
----------------------------------------
Num timesteps: 860400
Best mean reward: 0.49 - Last mean reward per episode: 0.36
Num timesteps: 861600
Best mean reward: 0.49 - Last mean reward per episode: 0.33
Num timesteps: 862800
Best mean reward: 0.49 - Last mean reward per episode: 0.41
Num timesteps: 864000
Best mean reward: 0.49 - Last mean reward per episode: 0.28
Num timesteps: 865200
Best mean reward: 0.49 - Last mean reward per episode: 0.36
Num timesteps: 866400
Best mean reward: 0.49 - Last mean reward per episode: 0.39
Num timesteps: 867600
Best mean reward: 0.49 - Last mean reward per episode: 0.43
Num timesteps: 868800
Best mean reward: 0.49 - Last mean reward per episode: 0.31
Num timesteps: 870000
Best mean reward: 0.49 - Last mean reward per episode: 0.33
Num timesteps: 871200
Best mean reward: 0.49 - Last mean reward per episode: 0.40
Num timesteps: 872400
Best mean reward: 0.49 - Last mean reward per episode: 0.42
Num timesteps: 873600
Best mean reward: 0.49 - Last mean reward per episode: 0.41
Num timesteps: 874800
Best mean reward: 0.49 - Last mean reward per episode: 0.45
Num timesteps: 876000
Best mean reward: 0.49 - Last mean reward per episode: 0.47
Num timesteps: 877200
Best mean reward: 0.49 - Last mean reward per episode: 0.39
Num timesteps: 878400
Best mean reward: 0.49 - Last mean reward per episode: 0.34
Num timesteps: 879600
Best mean reward: 0.49 - Last mean reward per episode: 0.34
Num timesteps: 880800
Best mean reward: 0.49 - Last mean reward per episode: 0.35
Num timesteps: 882000
Best mean reward: 0.49 - Last mean reward per episode: 0.36
Num timesteps: 883200
Best mean reward: 0.49 - Last mean reward per episode: 0.46
Num timesteps: 884400
Best mean reward: 0.49 - Last mean reward per episode: 0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.37       |
|    ep_rew_mean          | 0.36800003 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 36         |
|    time_elapsed         | 283462     |
|    total_timesteps      | 884736     |
| train/                  |            |
|    approx_kl            | 0.85579187 |
|    clip_fraction        | 0.748      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.563     |
|    explained_variance   | 0.0812     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0746    |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.1       |
|    std                  | 0.283      |
|    value_loss           | 0.17       |
----------------------------------------
Num timesteps: 885600
Best mean reward: 0.49 - Last mean reward per episode: 0.43
Num timesteps: 886800
Best mean reward: 0.49 - Last mean reward per episode: 0.42
Num timesteps: 888000
Best mean reward: 0.49 - Last mean reward per episode: 0.47
Num timesteps: 889200
Best mean reward: 0.49 - Last mean reward per episode: 0.29
Num timesteps: 890400
Best mean reward: 0.49 - Last mean reward per episode: 0.43
Num timesteps: 891600
Best mean reward: 0.49 - Last mean reward per episode: 0.37
Num timesteps: 892800
Best mean reward: 0.49 - Last mean reward per episode: 0.40
Num timesteps: 894000
Best mean reward: 0.49 - Last mean reward per episode: 0.40
Num timesteps: 895200
Best mean reward: 0.49 - Last mean reward per episode: 0.44
Num timesteps: 896400
Best mean reward: 0.49 - Last mean reward per episode: 0.48
Num timesteps: 897600
Best mean reward: 0.49 - Last mean reward per episode: 0.42
Num timesteps: 898800
Best mean reward: 0.49 - Last mean reward per episode: 0.35
Num timesteps: 900000
Best mean reward: 0.49 - Last mean reward per episode: 0.40
Num timesteps: 901200
Best mean reward: 0.49 - Last mean reward per episode: 0.36
Num timesteps: 902400
Best mean reward: 0.49 - Last mean reward per episode: 0.44
Num timesteps: 903600
Best mean reward: 0.49 - Last mean reward per episode: 0.40
Num timesteps: 904800
Best mean reward: 0.49 - Last mean reward per episode: 0.35
Num timesteps: 906000
Best mean reward: 0.49 - Last mean reward per episode: 0.40
Num timesteps: 907200
Best mean reward: 0.49 - Last mean reward per episode: 0.48
Num timesteps: 908400
Best mean reward: 0.49 - Last mean reward per episode: 0.41
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.67       |
|    ep_rew_mean          | 0.509      |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 37         |
|    time_elapsed         | 291254     |
|    total_timesteps      | 909312     |
| train/                  |            |
|    approx_kl            | 0.91331774 |
|    clip_fraction        | 0.758      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.411     |
|    explained_variance   | 0.0632     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.07      |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0965    |
|    std                  | 0.269      |
|    value_loss           | 0.173      |
----------------------------------------
Num timesteps: 909600
Best mean reward: 0.49 - Last mean reward per episode: 0.41
Num timesteps: 910800
Best mean reward: 0.49 - Last mean reward per episode: 0.42
Num timesteps: 912000
Best mean reward: 0.49 - Last mean reward per episode: 0.42
Num timesteps: 913200
Best mean reward: 0.49 - Last mean reward per episode: 0.42
Num timesteps: 914400
Best mean reward: 0.49 - Last mean reward per episode: 0.39
Num timesteps: 915600
Best mean reward: 0.49 - Last mean reward per episode: 0.44
Num timesteps: 916800
Best mean reward: 0.49 - Last mean reward per episode: 0.44
Num timesteps: 918000
Best mean reward: 0.49 - Last mean reward per episode: 0.43
Num timesteps: 919200
Best mean reward: 0.49 - Last mean reward per episode: 0.38
Num timesteps: 920400
Best mean reward: 0.49 - Last mean reward per episode: 0.38
Num timesteps: 921600
Best mean reward: 0.49 - Last mean reward per episode: 0.36
Num timesteps: 922800
Best mean reward: 0.49 - Last mean reward per episode: 0.46
Num timesteps: 924000
Best mean reward: 0.49 - Last mean reward per episode: 0.35
Num timesteps: 925200
Best mean reward: 0.49 - Last mean reward per episode: 0.40
Num timesteps: 926400
Best mean reward: 0.49 - Last mean reward per episode: 0.38
Num timesteps: 927600
Best mean reward: 0.49 - Last mean reward per episode: 0.46
Num timesteps: 928800
Best mean reward: 0.49 - Last mean reward per episode: 0.42
Num timesteps: 930000
Best mean reward: 0.49 - Last mean reward per episode: 0.48
Num timesteps: 931200
Best mean reward: 0.49 - Last mean reward per episode: 0.52
Saving new best model to models/train_stack2/best_model
Num timesteps: 932400
Best mean reward: 0.52 - Last mean reward per episode: 0.43
Num timesteps: 933600
Best mean reward: 0.52 - Last mean reward per episode: 0.41
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.67       |
|    ep_rew_mean          | 0.39400005 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 38         |
|    time_elapsed         | 299059     |
|    total_timesteps      | 933888     |
| train/                  |            |
|    approx_kl            | 1.0801692  |
|    clip_fraction        | 0.767      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.265     |
|    explained_variance   | 0.0838     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0737    |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.096     |
|    std                  | 0.256      |
|    value_loss           | 0.17       |
----------------------------------------
Num timesteps: 934800
Best mean reward: 0.52 - Last mean reward per episode: 0.44
Num timesteps: 936000
Best mean reward: 0.52 - Last mean reward per episode: 0.37
Num timesteps: 937200
Best mean reward: 0.52 - Last mean reward per episode: 0.35
Num timesteps: 938400
Best mean reward: 0.52 - Last mean reward per episode: 0.43
Num timesteps: 939600
Best mean reward: 0.52 - Last mean reward per episode: 0.37
Num timesteps: 940800
Best mean reward: 0.52 - Last mean reward per episode: 0.44
Num timesteps: 942000
Best mean reward: 0.52 - Last mean reward per episode: 0.35
Num timesteps: 943200
Best mean reward: 0.52 - Last mean reward per episode: 0.44
Num timesteps: 944400
Best mean reward: 0.52 - Last mean reward per episode: 0.40
Num timesteps: 945600
Best mean reward: 0.52 - Last mean reward per episode: 0.38
Num timesteps: 946800
Best mean reward: 0.52 - Last mean reward per episode: 0.43
Num timesteps: 948000
Best mean reward: 0.52 - Last mean reward per episode: 0.44
Num timesteps: 949200
Best mean reward: 0.52 - Last mean reward per episode: 0.38
Num timesteps: 950400
Best mean reward: 0.52 - Last mean reward per episode: 0.40
Num timesteps: 951600
Best mean reward: 0.52 - Last mean reward per episode: 0.44
Num timesteps: 952800
Best mean reward: 0.52 - Last mean reward per episode: 0.42
Num timesteps: 954000
Best mean reward: 0.52 - Last mean reward per episode: 0.40
Num timesteps: 955200
Best mean reward: 0.52 - Last mean reward per episode: 0.42
Num timesteps: 956400
Best mean reward: 0.52 - Last mean reward per episode: 0.46
Num timesteps: 957600
Best mean reward: 0.52 - Last mean reward per episode: 0.42
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 6.68      |
|    ep_rew_mean          | 0.299     |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 39        |
|    time_elapsed         | 306874    |
|    total_timesteps      | 958464    |
| train/                  |           |
|    approx_kl            | 1.0355583 |
|    clip_fraction        | 0.774     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.115    |
|    explained_variance   | 0.0774    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0644   |
|    n_updates            | 380       |
|    policy_gradient_loss | -0.0883   |
|    std                  | 0.243     |
|    value_loss           | 0.172     |
---------------------------------------
Num timesteps: 958800
Best mean reward: 0.52 - Last mean reward per episode: 0.42
Num timesteps: 960000
Best mean reward: 0.52 - Last mean reward per episode: 0.45
Num timesteps: 961200
Best mean reward: 0.52 - Last mean reward per episode: 0.37
Num timesteps: 962400
Best mean reward: 0.52 - Last mean reward per episode: 0.41
Num timesteps: 963600
Best mean reward: 0.52 - Last mean reward per episode: 0.36
Num timesteps: 964800
Best mean reward: 0.52 - Last mean reward per episode: 0.50
Num timesteps: 966000
Best mean reward: 0.52 - Last mean reward per episode: 0.36
Num timesteps: 967200
Best mean reward: 0.52 - Last mean reward per episode: 0.47
Num timesteps: 968400
Best mean reward: 0.52 - Last mean reward per episode: 0.34
Num timesteps: 969600
Best mean reward: 0.52 - Last mean reward per episode: 0.45
Num timesteps: 970800
Best mean reward: 0.52 - Last mean reward per episode: 0.41
Num timesteps: 972000
Best mean reward: 0.52 - Last mean reward per episode: 0.36
Num timesteps: 973200
Best mean reward: 0.52 - Last mean reward per episode: 0.37
Num timesteps: 974400
Best mean reward: 0.52 - Last mean reward per episode: 0.35
Num timesteps: 975600
Best mean reward: 0.52 - Last mean reward per episode: 0.40
Num timesteps: 976800
Best mean reward: 0.52 - Last mean reward per episode: 0.36
Num timesteps: 978000
Best mean reward: 0.52 - Last mean reward per episode: 0.44
Num timesteps: 979200
Best mean reward: 0.52 - Last mean reward per episode: 0.38
Num timesteps: 980400
Best mean reward: 0.52 - Last mean reward per episode: 0.39
Num timesteps: 981600
Best mean reward: 0.52 - Last mean reward per episode: 0.40
Num timesteps: 982800
Best mean reward: 0.52 - Last mean reward per episode: 0.48
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.11       |
|    ep_rew_mean          | 0.36500004 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 40         |
|    time_elapsed         | 314696     |
|    total_timesteps      | 983040     |
| train/                  |            |
|    approx_kl            | 1.060366   |
|    clip_fraction        | 0.77       |
|    clip_range           | 0.2        |
|    entropy_loss         | 0.0367     |
|    explained_variance   | 0.0736     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0651    |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0874    |
|    std                  | 0.231      |
|    value_loss           | 0.172      |
----------------------------------------
Num timesteps: 984000
Best mean reward: 0.52 - Last mean reward per episode: 0.48
Num timesteps: 985200
Best mean reward: 0.52 - Last mean reward per episode: 0.39
Num timesteps: 986400
Best mean reward: 0.52 - Last mean reward per episode: 0.45
Num timesteps: 987600
Best mean reward: 0.52 - Last mean reward per episode: 0.40
Num timesteps: 988800
Best mean reward: 0.52 - Last mean reward per episode: 0.41
Num timesteps: 990000
Best mean reward: 0.52 - Last mean reward per episode: 0.42
Num timesteps: 991200
Best mean reward: 0.52 - Last mean reward per episode: 0.39
Num timesteps: 992400
Best mean reward: 0.52 - Last mean reward per episode: 0.54
Saving new best model to models/train_stack2/best_model
Num timesteps: 993600
Best mean reward: 0.54 - Last mean reward per episode: 0.45
Num timesteps: 994800
Best mean reward: 0.54 - Last mean reward per episode: 0.48
Num timesteps: 996000
Best mean reward: 0.54 - Last mean reward per episode: 0.34
Num timesteps: 997200
Best mean reward: 0.54 - Last mean reward per episode: 0.53
Num timesteps: 998400
Best mean reward: 0.54 - Last mean reward per episode: 0.46
Num timesteps: 999600
Best mean reward: 0.54 - Last mean reward per episode: 0.48
Num timesteps: 1000800
Best mean reward: 0.54 - Last mean reward per episode: 0.49
Num timesteps: 1002000
Best mean reward: 0.54 - Last mean reward per episode: 0.39
Num timesteps: 1003200
Best mean reward: 0.54 - Last mean reward per episode: 0.43
Num timesteps: 1004400
Best mean reward: 0.54 - Last mean reward per episode: 0.55
Saving new best model to models/train_stack2/best_model
Num timesteps: 1005600
Best mean reward: 0.55 - Last mean reward per episode: 0.46
Num timesteps: 1006800
Best mean reward: 0.55 - Last mean reward per episode: 0.48
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.94       |
|    ep_rew_mean          | 0.38800004 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 41         |
|    time_elapsed         | 322525     |
|    total_timesteps      | 1007616    |
| train/                  |            |
|    approx_kl            | 1.1357352  |
|    clip_fraction        | 0.779      |
|    clip_range           | 0.2        |
|    entropy_loss         | 0.19       |
|    explained_variance   | 0.07       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0645    |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0849    |
|    std                  | 0.219      |
|    value_loss           | 0.17       |
----------------------------------------
Num timesteps: 1008000
Best mean reward: 0.55 - Last mean reward per episode: 0.38
Num timesteps: 1009200
Best mean reward: 0.55 - Last mean reward per episode: 0.45
Num timesteps: 1010400
Best mean reward: 0.55 - Last mean reward per episode: 0.49
Num timesteps: 1011600
Best mean reward: 0.55 - Last mean reward per episode: 0.49
Num timesteps: 1012800
Best mean reward: 0.55 - Last mean reward per episode: 0.52
Num timesteps: 1014000
Best mean reward: 0.55 - Last mean reward per episode: 0.49
Num timesteps: 1015200
Best mean reward: 0.55 - Last mean reward per episode: 0.51
Num timesteps: 1016400
Best mean reward: 0.55 - Last mean reward per episode: 0.39
Num timesteps: 1017600
Best mean reward: 0.55 - Last mean reward per episode: 0.48
Num timesteps: 1018800
Best mean reward: 0.55 - Last mean reward per episode: 0.42
Num timesteps: 1020000
Best mean reward: 0.55 - Last mean reward per episode: 0.51
Num timesteps: 1021200
Best mean reward: 0.55 - Last mean reward per episode: 0.45
Num timesteps: 1022400
Best mean reward: 0.55 - Last mean reward per episode: 0.50
Num timesteps: 1023600
Best mean reward: 0.55 - Last mean reward per episode: 0.45
Num timesteps: 1024800
Best mean reward: 0.55 - Last mean reward per episode: 0.41
Num timesteps: 1026000
Best mean reward: 0.55 - Last mean reward per episode: 0.47
Num timesteps: 1027200
Best mean reward: 0.55 - Last mean reward per episode: 0.46
Num timesteps: 1028400
Best mean reward: 0.55 - Last mean reward per episode: 0.43
Num timesteps: 1029600
Best mean reward: 0.55 - Last mean reward per episode: 0.45
Num timesteps: 1030800
Best mean reward: 0.55 - Last mean reward per episode: 0.40
Num timesteps: 1032000
Best mean reward: 0.55 - Last mean reward per episode: 0.54
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 6.97      |
|    ep_rew_mean          | 0.521     |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 42        |
|    time_elapsed         | 330320    |
|    total_timesteps      | 1032192   |
| train/                  |           |
|    approx_kl            | 1.2456979 |
|    clip_fraction        | 0.788     |
|    clip_range           | 0.2       |
|    entropy_loss         | 0.346     |
|    explained_variance   | 0.074     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0515   |
|    n_updates            | 410       |
|    policy_gradient_loss | -0.0859   |
|    std                  | 0.208     |
|    value_loss           | 0.171     |
---------------------------------------
Num timesteps: 1033200
Best mean reward: 0.55 - Last mean reward per episode: 0.48
Num timesteps: 1034400
Best mean reward: 0.55 - Last mean reward per episode: 0.56
Saving new best model to models/train_stack2/best_model
Num timesteps: 1035600
Best mean reward: 0.56 - Last mean reward per episode: 0.49
Num timesteps: 1036800
Best mean reward: 0.56 - Last mean reward per episode: 0.35
Num timesteps: 1038000
Best mean reward: 0.56 - Last mean reward per episode: 0.48
Num timesteps: 1039200
Best mean reward: 0.56 - Last mean reward per episode: 0.37
Num timesteps: 1040400
Best mean reward: 0.56 - Last mean reward per episode: 0.43
Num timesteps: 1041600
Best mean reward: 0.56 - Last mean reward per episode: 0.44
Num timesteps: 1042800
Best mean reward: 0.56 - Last mean reward per episode: 0.47
Num timesteps: 1044000
Best mean reward: 0.56 - Last mean reward per episode: 0.45
Num timesteps: 1045200
Best mean reward: 0.56 - Last mean reward per episode: 0.43
Num timesteps: 1046400
Best mean reward: 0.56 - Last mean reward per episode: 0.45
Num timesteps: 1047600
Best mean reward: 0.56 - Last mean reward per episode: 0.47
Num timesteps: 1048800
Best mean reward: 0.56 - Last mean reward per episode: 0.44
Num timesteps: 1050000
Best mean reward: 0.56 - Last mean reward per episode: 0.45
Num timesteps: 1051200
Best mean reward: 0.56 - Last mean reward per episode: 0.43
Num timesteps: 1052400
Best mean reward: 0.56 - Last mean reward per episode: 0.48
Num timesteps: 1053600
Best mean reward: 0.56 - Last mean reward per episode: 0.34
Num timesteps: 1054800
Best mean reward: 0.56 - Last mean reward per episode: 0.44
Num timesteps: 1056000
Best mean reward: 0.56 - Last mean reward per episode: 0.48
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.29       |
|    ep_rew_mean          | 0.41600007 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 43         |
|    time_elapsed         | 338120     |
|    total_timesteps      | 1056768    |
| train/                  |            |
|    approx_kl            | 1.3327003  |
|    clip_fraction        | 0.788      |
|    clip_range           | 0.2        |
|    entropy_loss         | 0.521      |
|    explained_variance   | 0.101      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0638    |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0826    |
|    std                  | 0.196      |
|    value_loss           | 0.165      |
----------------------------------------
Num timesteps: 1057200
Best mean reward: 0.56 - Last mean reward per episode: 0.40
Num timesteps: 1058400
Best mean reward: 0.56 - Last mean reward per episode: 0.44
Num timesteps: 1059600
Best mean reward: 0.56 - Last mean reward per episode: 0.47
Num timesteps: 1060800
Best mean reward: 0.56 - Last mean reward per episode: 0.43
Num timesteps: 1062000
Best mean reward: 0.56 - Last mean reward per episode: 0.50
Num timesteps: 1063200
Best mean reward: 0.56 - Last mean reward per episode: 0.39
Num timesteps: 1064400
Best mean reward: 0.56 - Last mean reward per episode: 0.47
Num timesteps: 1065600
Best mean reward: 0.56 - Last mean reward per episode: 0.48
Num timesteps: 1066800
Best mean reward: 0.56 - Last mean reward per episode: 0.48
Num timesteps: 1068000
Best mean reward: 0.56 - Last mean reward per episode: 0.50
Num timesteps: 1069200
Best mean reward: 0.56 - Last mean reward per episode: 0.41
Num timesteps: 1070400
Best mean reward: 0.56 - Last mean reward per episode: 0.46
Num timesteps: 1071600
Best mean reward: 0.56 - Last mean reward per episode: 0.48
Num timesteps: 1072800
Best mean reward: 0.56 - Last mean reward per episode: 0.43
Num timesteps: 1074000
Best mean reward: 0.56 - Last mean reward per episode: 0.44
Num timesteps: 1075200
Best mean reward: 0.56 - Last mean reward per episode: 0.47
Num timesteps: 1076400
Best mean reward: 0.56 - Last mean reward per episode: 0.52
Num timesteps: 1077600
Best mean reward: 0.56 - Last mean reward per episode: 0.56
Num timesteps: 1078800
Best mean reward: 0.56 - Last mean reward per episode: 0.45
Num timesteps: 1080000
Best mean reward: 0.56 - Last mean reward per episode: 0.54
Num timesteps: 1081200
Best mean reward: 0.56 - Last mean reward per episode: 0.44
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 7.01       |
|    ep_rew_mean          | 0.39300004 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 44         |
|    time_elapsed         | 345898     |
|    total_timesteps      | 1081344    |
| train/                  |            |
|    approx_kl            | 1.270708   |
|    clip_fraction        | 0.796      |
|    clip_range           | 0.2        |
|    entropy_loss         | 0.688      |
|    explained_variance   | 0.0863     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0611    |
|    n_updates            | 430        |
|    policy_gradient_loss | -0.0759    |
|    std                  | 0.186      |
|    value_loss           | 0.166      |
----------------------------------------
Num timesteps: 1082400
Best mean reward: 0.56 - Last mean reward per episode: 0.50
Num timesteps: 1083600
Best mean reward: 0.56 - Last mean reward per episode: 0.49
Num timesteps: 1084800
Best mean reward: 0.56 - Last mean reward per episode: 0.44
Num timesteps: 1086000
Best mean reward: 0.56 - Last mean reward per episode: 0.55
Num timesteps: 1087200
Best mean reward: 0.56 - Last mean reward per episode: 0.41
Num timesteps: 1088400
Best mean reward: 0.56 - Last mean reward per episode: 0.39
Num timesteps: 1089600
Best mean reward: 0.56 - Last mean reward per episode: 0.55
Num timesteps: 1090800
Best mean reward: 0.56 - Last mean reward per episode: 0.50
Num timesteps: 1092000
Best mean reward: 0.56 - Last mean reward per episode: 0.40
Num timesteps: 1093200
Best mean reward: 0.56 - Last mean reward per episode: 0.49
Num timesteps: 1094400
Best mean reward: 0.56 - Last mean reward per episode: 0.44
Num timesteps: 1095600
Best mean reward: 0.56 - Last mean reward per episode: 0.47
Num timesteps: 1096800
Best mean reward: 0.56 - Last mean reward per episode: 0.38
Num timesteps: 1098000
Best mean reward: 0.56 - Last mean reward per episode: 0.42
Num timesteps: 1099200
Best mean reward: 0.56 - Last mean reward per episode: 0.52
Num timesteps: 1100400
Best mean reward: 0.56 - Last mean reward per episode: 0.41
Num timesteps: 1101600
Best mean reward: 0.56 - Last mean reward per episode: 0.44
Num timesteps: 1102800
Best mean reward: 0.56 - Last mean reward per episode: 0.37
Num timesteps: 1104000
Best mean reward: 0.56 - Last mean reward per episode: 0.41
Num timesteps: 1105200
Best mean reward: 0.56 - Last mean reward per episode: 0.41
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 7.49       |
|    ep_rew_mean          | 0.38400006 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 45         |
|    time_elapsed         | 353694     |
|    total_timesteps      | 1105920    |
| train/                  |            |
|    approx_kl            | 1.4552702  |
|    clip_fraction        | 0.801      |
|    clip_range           | 0.2        |
|    entropy_loss         | 0.838      |
|    explained_variance   | 0.098      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0627    |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0748    |
|    std                  | 0.177      |
|    value_loss           | 0.165      |
----------------------------------------
Num timesteps: 1106400
Best mean reward: 0.56 - Last mean reward per episode: 0.48
Num timesteps: 1107600
Best mean reward: 0.56 - Last mean reward per episode: 0.42
Num timesteps: 1108800
Best mean reward: 0.56 - Last mean reward per episode: 0.47
Num timesteps: 1110000
Best mean reward: 0.56 - Last mean reward per episode: 0.58
Saving new best model to models/train_stack2/best_model
Num timesteps: 1111200
Best mean reward: 0.58 - Last mean reward per episode: 0.53
Num timesteps: 1112400
Best mean reward: 0.58 - Last mean reward per episode: 0.50
Num timesteps: 1113600
Best mean reward: 0.58 - Last mean reward per episode: 0.55
Num timesteps: 1114800
Best mean reward: 0.58 - Last mean reward per episode: 0.43
Num timesteps: 1116000
Best mean reward: 0.58 - Last mean reward per episode: 0.46
Num timesteps: 1117200
Best mean reward: 0.58 - Last mean reward per episode: 0.36
Num timesteps: 1118400
Best mean reward: 0.58 - Last mean reward per episode: 0.45
Num timesteps: 1119600
Best mean reward: 0.58 - Last mean reward per episode: 0.54
Num timesteps: 1120800
Best mean reward: 0.58 - Last mean reward per episode: 0.47
Num timesteps: 1122000
Best mean reward: 0.58 - Last mean reward per episode: 0.45
Num timesteps: 1123200
Best mean reward: 0.58 - Last mean reward per episode: 0.43
Num timesteps: 1124400
Best mean reward: 0.58 - Last mean reward per episode: 0.47
Num timesteps: 1125600
Best mean reward: 0.58 - Last mean reward per episode: 0.55
Num timesteps: 1126800
Best mean reward: 0.58 - Last mean reward per episode: 0.47
Num timesteps: 1128000
Best mean reward: 0.58 - Last mean reward per episode: 0.47
Num timesteps: 1129200
Best mean reward: 0.58 - Last mean reward per episode: 0.41
Num timesteps: 1130400
Best mean reward: 0.58 - Last mean reward per episode: 0.46
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 7.1       |
|    ep_rew_mean          | 0.5       |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 46        |
|    time_elapsed         | 361506    |
|    total_timesteps      | 1130496   |
| train/                  |           |
|    approx_kl            | 1.4697586 |
|    clip_fraction        | 0.804     |
|    clip_range           | 0.2       |
|    entropy_loss         | 0.991     |
|    explained_variance   | 0.0893    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0301   |
|    n_updates            | 450       |
|    policy_gradient_loss | -0.0621   |
|    std                  | 0.168     |
|    value_loss           | 0.168     |
---------------------------------------
Num timesteps: 1131600
Best mean reward: 0.58 - Last mean reward per episode: 0.53
Num timesteps: 1132800
Best mean reward: 0.58 - Last mean reward per episode: 0.44
Num timesteps: 1134000
Best mean reward: 0.58 - Last mean reward per episode: 0.46
Num timesteps: 1135200
Best mean reward: 0.58 - Last mean reward per episode: 0.41
Num timesteps: 1136400
Best mean reward: 0.58 - Last mean reward per episode: 0.53
Num timesteps: 1137600
Best mean reward: 0.58 - Last mean reward per episode: 0.49
Num timesteps: 1138800
Best mean reward: 0.58 - Last mean reward per episode: 0.51
Num timesteps: 1140000
Best mean reward: 0.58 - Last mean reward per episode: 0.42
Num timesteps: 1141200
Best mean reward: 0.58 - Last mean reward per episode: 0.42
Num timesteps: 1142400
Best mean reward: 0.58 - Last mean reward per episode: 0.48
Num timesteps: 1143600
Best mean reward: 0.58 - Last mean reward per episode: 0.56
Num timesteps: 1144800
Best mean reward: 0.58 - Last mean reward per episode: 0.45
Num timesteps: 1146000
Best mean reward: 0.58 - Last mean reward per episode: 0.46
Num timesteps: 1147200
Best mean reward: 0.58 - Last mean reward per episode: 0.42
Num timesteps: 1148400
Best mean reward: 0.58 - Last mean reward per episode: 0.46
Num timesteps: 1149600
Best mean reward: 0.58 - Last mean reward per episode: 0.41
Num timesteps: 1150800
Best mean reward: 0.58 - Last mean reward per episode: 0.54
Num timesteps: 1152000
Best mean reward: 0.58 - Last mean reward per episode: 0.43
Num timesteps: 1153200
Best mean reward: 0.58 - Last mean reward per episode: 0.47
Num timesteps: 1154400
Best mean reward: 0.58 - Last mean reward per episode: 0.57
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 7.33      |
|    ep_rew_mean          | 0.513     |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 47        |
|    time_elapsed         | 369322    |
|    total_timesteps      | 1155072   |
| train/                  |           |
|    approx_kl            | 1.9412905 |
|    clip_fraction        | 0.803     |
|    clip_range           | 0.2       |
|    entropy_loss         | 1.14      |
|    explained_variance   | 0.105     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0757   |
|    n_updates            | 460       |
|    policy_gradient_loss | -0.0663   |
|    std                  | 0.159     |
|    value_loss           | 0.165     |
---------------------------------------
Num timesteps: 1155600
Best mean reward: 0.58 - Last mean reward per episode: 0.43
Num timesteps: 1156800
Best mean reward: 0.58 - Last mean reward per episode: 0.43
Num timesteps: 1158000
Best mean reward: 0.58 - Last mean reward per episode: 0.50
Num timesteps: 1159200
Best mean reward: 0.58 - Last mean reward per episode: 0.57
Num timesteps: 1160400
Best mean reward: 0.58 - Last mean reward per episode: 0.46
Num timesteps: 1161600
Best mean reward: 0.58 - Last mean reward per episode: 0.55
Num timesteps: 1162800
Best mean reward: 0.58 - Last mean reward per episode: 0.56
Num timesteps: 1164000
Best mean reward: 0.58 - Last mean reward per episode: 0.47
Num timesteps: 1165200
Best mean reward: 0.58 - Last mean reward per episode: 0.45
Num timesteps: 1166400
Best mean reward: 0.58 - Last mean reward per episode: 0.45
Num timesteps: 1167600
Best mean reward: 0.58 - Last mean reward per episode: 0.47
Num timesteps: 1168800
Best mean reward: 0.58 - Last mean reward per episode: 0.52
Num timesteps: 1170000
Best mean reward: 0.58 - Last mean reward per episode: 0.38
Num timesteps: 1171200
Best mean reward: 0.58 - Last mean reward per episode: 0.44
Num timesteps: 1172400
Best mean reward: 0.58 - Last mean reward per episode: 0.59
Saving new best model to models/train_stack2/best_model
Num timesteps: 1173600
Best mean reward: 0.59 - Last mean reward per episode: 0.48
Num timesteps: 1174800
Best mean reward: 0.59 - Last mean reward per episode: 0.51
Num timesteps: 1176000
Best mean reward: 0.59 - Last mean reward per episode: 0.53
Num timesteps: 1177200
Best mean reward: 0.59 - Last mean reward per episode: 0.54
Num timesteps: 1178400
Best mean reward: 0.59 - Last mean reward per episode: 0.49
Num timesteps: 1179600
Best mean reward: 0.59 - Last mean reward per episode: 0.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 7.04       |
|    ep_rew_mean          | 0.49500003 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 48         |
|    time_elapsed         | 377099     |
|    total_timesteps      | 1179648    |
| train/                  |            |
|    approx_kl            | 1.5888535  |
|    clip_fraction        | 0.812      |
|    clip_range           | 0.2        |
|    entropy_loss         | 1.3        |
|    explained_variance   | 0.0848     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0594    |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.0607    |
|    std                  | 0.151      |
|    value_loss           | 0.166      |
----------------------------------------
Num timesteps: 1180800
Best mean reward: 0.59 - Last mean reward per episode: 0.40
Num timesteps: 1182000
Best mean reward: 0.59 - Last mean reward per episode: 0.53
Num timesteps: 1183200
Best mean reward: 0.59 - Last mean reward per episode: 0.49
Num timesteps: 1184400
Best mean reward: 0.59 - Last mean reward per episode: 0.43
Num timesteps: 1185600
Best mean reward: 0.59 - Last mean reward per episode: 0.44
Num timesteps: 1186800
Best mean reward: 0.59 - Last mean reward per episode: 0.45
Num timesteps: 1188000
Best mean reward: 0.59 - Last mean reward per episode: 0.49
Num timesteps: 1189200
Best mean reward: 0.59 - Last mean reward per episode: 0.49
Num timesteps: 1190400
Best mean reward: 0.59 - Last mean reward per episode: 0.39
Num timesteps: 1191600
Best mean reward: 0.59 - Last mean reward per episode: 0.51
Num timesteps: 1192800
Best mean reward: 0.59 - Last mean reward per episode: 0.41
Num timesteps: 1194000
Best mean reward: 0.59 - Last mean reward per episode: 0.44
Num timesteps: 1195200
Best mean reward: 0.59 - Last mean reward per episode: 0.55
Num timesteps: 1196400
Best mean reward: 0.59 - Last mean reward per episode: 0.50
Num timesteps: 1197600
Best mean reward: 0.59 - Last mean reward per episode: 0.50
Num timesteps: 1198800
Best mean reward: 0.59 - Last mean reward per episode: 0.42
Num timesteps: 1200000
Best mean reward: 0.59 - Last mean reward per episode: 0.51
Num timesteps: 1201200
Best mean reward: 0.59 - Last mean reward per episode: 0.48
Num timesteps: 1202400
Best mean reward: 0.59 - Last mean reward per episode: 0.52
Num timesteps: 1203600
Best mean reward: 0.59 - Last mean reward per episode: 0.51
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 7.59       |
|    ep_rew_mean          | 0.45900002 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 49         |
|    time_elapsed         | 384917     |
|    total_timesteps      | 1204224    |
| train/                  |            |
|    approx_kl            | 1.7029167  |
|    clip_fraction        | 0.816      |
|    clip_range           | 0.2        |
|    entropy_loss         | 1.45       |
|    explained_variance   | 0.0795     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0441    |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0482    |
|    std                  | 0.144      |
|    value_loss           | 0.166      |
----------------------------------------
Num timesteps: 1204800
Best mean reward: 0.59 - Last mean reward per episode: 0.50
Num timesteps: 1206000
Best mean reward: 0.59 - Last mean reward per episode: 0.51
Num timesteps: 1207200
Best mean reward: 0.59 - Last mean reward per episode: 0.43
Num timesteps: 1208400
Best mean reward: 0.59 - Last mean reward per episode: 0.50
Num timesteps: 1209600
Best mean reward: 0.59 - Last mean reward per episode: 0.38
Num timesteps: 1210800
Best mean reward: 0.59 - Last mean reward per episode: 0.42
Num timesteps: 1212000
Best mean reward: 0.59 - Last mean reward per episode: 0.51
Num timesteps: 1213200
Best mean reward: 0.59 - Last mean reward per episode: 0.49
Num timesteps: 1214400
Best mean reward: 0.59 - Last mean reward per episode: 0.52
Num timesteps: 1215600
Best mean reward: 0.59 - Last mean reward per episode: 0.51
Num timesteps: 1216800
Best mean reward: 0.59 - Last mean reward per episode: 0.53
Num timesteps: 1218000
Best mean reward: 0.59 - Last mean reward per episode: 0.52
Num timesteps: 1219200
Best mean reward: 0.59 - Last mean reward per episode: 0.47
Num timesteps: 1220400
Best mean reward: 0.59 - Last mean reward per episode: 0.49
Num timesteps: 1221600
Best mean reward: 0.59 - Last mean reward per episode: 0.45
Num timesteps: 1222800
Best mean reward: 0.59 - Last mean reward per episode: 0.52
Num timesteps: 1224000
Best mean reward: 0.59 - Last mean reward per episode: 0.50
Num timesteps: 1225200
Best mean reward: 0.59 - Last mean reward per episode: 0.46
Num timesteps: 1226400
Best mean reward: 0.59 - Last mean reward per episode: 0.49
Num timesteps: 1227600
Best mean reward: 0.59 - Last mean reward per episode: 0.57
Num timesteps: 1228800
Best mean reward: 0.59 - Last mean reward per episode: 0.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 6.53      |
|    ep_rew_mean          | 0.455     |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 50        |
|    time_elapsed         | 392798    |
|    total_timesteps      | 1228800   |
| train/                  |           |
|    approx_kl            | 1.7449893 |
|    clip_fraction        | 0.817     |
|    clip_range           | 0.2       |
|    entropy_loss         | 1.6       |
|    explained_variance   | 0.0966    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0478   |
|    n_updates            | 490       |
|    policy_gradient_loss | -0.0615   |
|    std                  | 0.137     |
|    value_loss           | 0.166     |
---------------------------------------
Num timesteps: 1230000
Best mean reward: 0.59 - Last mean reward per episode: 0.44
Num timesteps: 1231200
Best mean reward: 0.59 - Last mean reward per episode: 0.58
Num timesteps: 1232400
Best mean reward: 0.59 - Last mean reward per episode: 0.47
Num timesteps: 1233600
Best mean reward: 0.59 - Last mean reward per episode: 0.58
Num timesteps: 1234800
Best mean reward: 0.59 - Last mean reward per episode: 0.52
Num timesteps: 1236000
Best mean reward: 0.59 - Last mean reward per episode: 0.45
Num timesteps: 1237200
Best mean reward: 0.59 - Last mean reward per episode: 0.49
Num timesteps: 1238400
Best mean reward: 0.59 - Last mean reward per episode: 0.44
Num timesteps: 1239600
Best mean reward: 0.59 - Last mean reward per episode: 0.47
Num timesteps: 1240800
Best mean reward: 0.59 - Last mean reward per episode: 0.57
Num timesteps: 1242000
Best mean reward: 0.59 - Last mean reward per episode: 0.44
Num timesteps: 1243200
Best mean reward: 0.59 - Last mean reward per episode: 0.54
Num timesteps: 1244400
Best mean reward: 0.59 - Last mean reward per episode: 0.46
Num timesteps: 1245600
Best mean reward: 0.59 - Last mean reward per episode: 0.53
Num timesteps: 1246800
Best mean reward: 0.59 - Last mean reward per episode: 0.41
Num timesteps: 1248000
Best mean reward: 0.59 - Last mean reward per episode: 0.51
Num timesteps: 1249200
Best mean reward: 0.59 - Last mean reward per episode: 0.51
Num timesteps: 1250400
Best mean reward: 0.59 - Last mean reward per episode: 0.55
Num timesteps: 1251600
Best mean reward: 0.59 - Last mean reward per episode: 0.36
Num timesteps: 1252800
Best mean reward: 0.59 - Last mean reward per episode: 0.43
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 7.09       |
|    ep_rew_mean          | 0.48000005 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 51         |
|    time_elapsed         | 400670     |
|    total_timesteps      | 1253376    |
| train/                  |            |
|    approx_kl            | 1.8792505  |
|    clip_fraction        | 0.822      |
|    clip_range           | 0.2        |
|    entropy_loss         | 1.75       |
|    explained_variance   | 0.1        |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0445    |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.0488    |
|    std                  | 0.13       |
|    value_loss           | 0.169      |
----------------------------------------
Num timesteps: 1254000
Best mean reward: 0.59 - Last mean reward per episode: 0.42
Num timesteps: 1255200
Best mean reward: 0.59 - Last mean reward per episode: 0.53
Num timesteps: 1256400
Best mean reward: 0.59 - Last mean reward per episode: 0.44
Num timesteps: 1257600
Best mean reward: 0.59 - Last mean reward per episode: 0.49
Num timesteps: 1258800
Best mean reward: 0.59 - Last mean reward per episode: 0.54
Num timesteps: 1260000
Best mean reward: 0.59 - Last mean reward per episode: 0.47
Num timesteps: 1261200
Best mean reward: 0.59 - Last mean reward per episode: 0.41
Num timesteps: 1262400
Best mean reward: 0.59 - Last mean reward per episode: 0.53
Num timesteps: 1263600
Best mean reward: 0.59 - Last mean reward per episode: 0.50
Num timesteps: 1264800
Best mean reward: 0.59 - Last mean reward per episode: 0.51
Num timesteps: 1266000
Best mean reward: 0.59 - Last mean reward per episode: 0.55
Num timesteps: 1267200
Best mean reward: 0.59 - Last mean reward per episode: 0.60
Saving new best model to models/train_stack2/best_model
Num timesteps: 1268400
Best mean reward: 0.60 - Last mean reward per episode: 0.47
Num timesteps: 1269600
Best mean reward: 0.60 - Last mean reward per episode: 0.52
Num timesteps: 1270800
Best mean reward: 0.60 - Last mean reward per episode: 0.43
Num timesteps: 1272000
Best mean reward: 0.60 - Last mean reward per episode: 0.36
Num timesteps: 1273200
Best mean reward: 0.60 - Last mean reward per episode: 0.50
Num timesteps: 1274400
Best mean reward: 0.60 - Last mean reward per episode: 0.50
Num timesteps: 1275600
Best mean reward: 0.60 - Last mean reward per episode: 0.42
Num timesteps: 1276800
Best mean reward: 0.60 - Last mean reward per episode: 0.51
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 7.19      |
|    ep_rew_mean          | 0.56      |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 52        |
|    time_elapsed         | 408535    |
|    total_timesteps      | 1277952   |
| train/                  |           |
|    approx_kl            | 1.8136927 |
|    clip_fraction        | 0.822     |
|    clip_range           | 0.2       |
|    entropy_loss         | 1.89      |
|    explained_variance   | 0.0952    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.049    |
|    n_updates            | 510       |
|    policy_gradient_loss | -0.0429   |
|    std                  | 0.124     |
|    value_loss           | 0.169     |
---------------------------------------
Num timesteps: 1278000
Best mean reward: 0.60 - Last mean reward per episode: 0.54
Num timesteps: 1279200
Best mean reward: 0.60 - Last mean reward per episode: 0.64
Saving new best model to models/train_stack2/best_model
Num timesteps: 1280400
Best mean reward: 0.64 - Last mean reward per episode: 0.40
Num timesteps: 1281600
Best mean reward: 0.64 - Last mean reward per episode: 0.44
Num timesteps: 1282800
Best mean reward: 0.64 - Last mean reward per episode: 0.48
Num timesteps: 1284000
Best mean reward: 0.64 - Last mean reward per episode: 0.49
Num timesteps: 1285200
Best mean reward: 0.64 - Last mean reward per episode: 0.38
Num timesteps: 1286400
Best mean reward: 0.64 - Last mean reward per episode: 0.51
Num timesteps: 1287600
Best mean reward: 0.64 - Last mean reward per episode: 0.54
Num timesteps: 1288800
Best mean reward: 0.64 - Last mean reward per episode: 0.47
Num timesteps: 1290000
Best mean reward: 0.64 - Last mean reward per episode: 0.52
Num timesteps: 1291200
Best mean reward: 0.64 - Last mean reward per episode: 0.56
Num timesteps: 1292400
Best mean reward: 0.64 - Last mean reward per episode: 0.54
Num timesteps: 1293600
Best mean reward: 0.64 - Last mean reward per episode: 0.45
Num timesteps: 1294800
Best mean reward: 0.64 - Last mean reward per episode: 0.42
Num timesteps: 1296000
Best mean reward: 0.64 - Last mean reward per episode: 0.53
Num timesteps: 1297200
Best mean reward: 0.64 - Last mean reward per episode: 0.47
Num timesteps: 1298400
Best mean reward: 0.64 - Last mean reward per episode: 0.50
Num timesteps: 1299600
Best mean reward: 0.64 - Last mean reward per episode: 0.53
Num timesteps: 1300800
Best mean reward: 0.64 - Last mean reward per episode: 0.45
Num timesteps: 1302000
Best mean reward: 0.64 - Last mean reward per episode: 0.49
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 7.47       |
|    ep_rew_mean          | 0.46600005 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 53         |
|    time_elapsed         | 416382     |
|    total_timesteps      | 1302528    |
| train/                  |            |
|    approx_kl            | 1.8328043  |
|    clip_fraction        | 0.822      |
|    clip_range           | 0.2        |
|    entropy_loss         | 2.02       |
|    explained_variance   | 0.0899     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0272    |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.041     |
|    std                  | 0.119      |
|    value_loss           | 0.17       |
----------------------------------------
Num timesteps: 1303200
Best mean reward: 0.64 - Last mean reward per episode: 0.57
Num timesteps: 1304400
Best mean reward: 0.64 - Last mean reward per episode: 0.65
Saving new best model to models/train_stack2/best_model
Num timesteps: 1305600
Best mean reward: 0.65 - Last mean reward per episode: 0.49
Num timesteps: 1306800
Best mean reward: 0.65 - Last mean reward per episode: 0.50
Num timesteps: 1308000
Best mean reward: 0.65 - Last mean reward per episode: 0.53
Num timesteps: 1309200
Best mean reward: 0.65 - Last mean reward per episode: 0.49
Num timesteps: 1310400
Best mean reward: 0.65 - Last mean reward per episode: 0.35
Num timesteps: 1311600
Best mean reward: 0.65 - Last mean reward per episode: 0.53
Num timesteps: 1312800
Best mean reward: 0.65 - Last mean reward per episode: 0.48
Num timesteps: 1314000
Best mean reward: 0.65 - Last mean reward per episode: 0.52
Num timesteps: 1315200
Best mean reward: 0.65 - Last mean reward per episode: 0.48
Num timesteps: 1316400
Best mean reward: 0.65 - Last mean reward per episode: 0.53
Num timesteps: 1317600
Best mean reward: 0.65 - Last mean reward per episode: 0.58
Num timesteps: 1318800
Best mean reward: 0.65 - Last mean reward per episode: 0.51
Num timesteps: 1320000
Best mean reward: 0.65 - Last mean reward per episode: 0.47
Num timesteps: 1321200
Best mean reward: 0.65 - Last mean reward per episode: 0.55
Num timesteps: 1322400
Best mean reward: 0.65 - Last mean reward per episode: 0.43
Num timesteps: 1323600
Best mean reward: 0.65 - Last mean reward per episode: 0.46
Num timesteps: 1324800
Best mean reward: 0.65 - Last mean reward per episode: 0.39
Num timesteps: 1326000
Best mean reward: 0.65 - Last mean reward per episode: 0.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.58       |
|    ep_rew_mean          | 0.49000004 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 54         |
|    time_elapsed         | 424256     |
|    total_timesteps      | 1327104    |
| train/                  |            |
|    approx_kl            | 2.4469125  |
|    clip_fraction        | 0.829      |
|    clip_range           | 0.2        |
|    entropy_loss         | 2.15       |
|    explained_variance   | 0.102      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0447    |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.0335    |
|    std                  | 0.114      |
|    value_loss           | 0.169      |
----------------------------------------
Num timesteps: 1327200
Best mean reward: 0.65 - Last mean reward per episode: 0.46
Num timesteps: 1328400
Best mean reward: 0.65 - Last mean reward per episode: 0.42
Num timesteps: 1329600
Best mean reward: 0.65 - Last mean reward per episode: 0.46
Num timesteps: 1330800
Best mean reward: 0.65 - Last mean reward per episode: 0.51
Num timesteps: 1332000
Best mean reward: 0.65 - Last mean reward per episode: 0.46
Num timesteps: 1333200
Best mean reward: 0.65 - Last mean reward per episode: 0.49
Num timesteps: 1334400
Best mean reward: 0.65 - Last mean reward per episode: 0.53
Num timesteps: 1335600
Best mean reward: 0.65 - Last mean reward per episode: 0.48
Num timesteps: 1336800
Best mean reward: 0.65 - Last mean reward per episode: 0.47
Num timesteps: 1338000
Best mean reward: 0.65 - Last mean reward per episode: 0.56
Num timesteps: 1339200
Best mean reward: 0.65 - Last mean reward per episode: 0.45
Num timesteps: 1340400
Best mean reward: 0.65 - Last mean reward per episode: 0.54
Num timesteps: 1341600
Best mean reward: 0.65 - Last mean reward per episode: 0.47
Num timesteps: 1342800
Best mean reward: 0.65 - Last mean reward per episode: 0.49
Num timesteps: 1344000
Best mean reward: 0.65 - Last mean reward per episode: 0.52
Num timesteps: 1345200
Best mean reward: 0.65 - Last mean reward per episode: 0.42
Num timesteps: 1346400
Best mean reward: 0.65 - Last mean reward per episode: 0.52
Num timesteps: 1347600
Best mean reward: 0.65 - Last mean reward per episode: 0.43
Num timesteps: 1348800
Best mean reward: 0.65 - Last mean reward per episode: 0.45
Num timesteps: 1350000
Best mean reward: 0.65 - Last mean reward per episode: 0.52
Num timesteps: 1351200
Best mean reward: 0.65 - Last mean reward per episode: 0.50
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 6.79      |
|    ep_rew_mean          | 0.577     |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 55        |
|    time_elapsed         | 432130    |
|    total_timesteps      | 1351680   |
| train/                  |           |
|    approx_kl            | 2.1411152 |
|    clip_fraction        | 0.833     |
|    clip_range           | 0.2       |
|    entropy_loss         | 2.28      |
|    explained_variance   | 0.0759    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0242   |
|    n_updates            | 540       |
|    policy_gradient_loss | -0.0219   |
|    std                  | 0.109     |
|    value_loss           | 0.171     |
---------------------------------------
Num timesteps: 1352400
Best mean reward: 0.65 - Last mean reward per episode: 0.46
Num timesteps: 1353600
Best mean reward: 0.65 - Last mean reward per episode: 0.44
Num timesteps: 1354800
Best mean reward: 0.65 - Last mean reward per episode: 0.52
Num timesteps: 1356000
Best mean reward: 0.65 - Last mean reward per episode: 0.43
Num timesteps: 1357200
Best mean reward: 0.65 - Last mean reward per episode: 0.51
Num timesteps: 1358400
Best mean reward: 0.65 - Last mean reward per episode: 0.54
Num timesteps: 1359600
Best mean reward: 0.65 - Last mean reward per episode: 0.47
Num timesteps: 1360800
Best mean reward: 0.65 - Last mean reward per episode: 0.47
Num timesteps: 1362000
Best mean reward: 0.65 - Last mean reward per episode: 0.52
Num timesteps: 1363200
Best mean reward: 0.65 - Last mean reward per episode: 0.40
Num timesteps: 1364400
Best mean reward: 0.65 - Last mean reward per episode: 0.53
Num timesteps: 1365600
Best mean reward: 0.65 - Last mean reward per episode: 0.53
Num timesteps: 1366800
Best mean reward: 0.65 - Last mean reward per episode: 0.41
Num timesteps: 1368000
Best mean reward: 0.65 - Last mean reward per episode: 0.43
Num timesteps: 1369200
Best mean reward: 0.65 - Last mean reward per episode: 0.45
Num timesteps: 1370400
Best mean reward: 0.65 - Last mean reward per episode: 0.55
Num timesteps: 1371600
Best mean reward: 0.65 - Last mean reward per episode: 0.59
Num timesteps: 1372800
Best mean reward: 0.65 - Last mean reward per episode: 0.57
Num timesteps: 1374000
Best mean reward: 0.65 - Last mean reward per episode: 0.50
Num timesteps: 1375200
Best mean reward: 0.65 - Last mean reward per episode: 0.44
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 5.85      |
|    ep_rew_mean          | 0.4440001 |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 56        |
|    time_elapsed         | 440006    |
|    total_timesteps      | 1376256   |
| train/                  |           |
|    approx_kl            | 2.0245965 |
|    clip_fraction        | 0.834     |
|    clip_range           | 0.2       |
|    entropy_loss         | 2.41      |
|    explained_variance   | 0.0881    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0294   |
|    n_updates            | 550       |
|    policy_gradient_loss | -0.0269   |
|    std                  | 0.104     |
|    value_loss           | 0.17      |
---------------------------------------
Num timesteps: 1376400
Best mean reward: 0.65 - Last mean reward per episode: 0.49
Num timesteps: 1377600
Best mean reward: 0.65 - Last mean reward per episode: 0.50
Num timesteps: 1378800
Best mean reward: 0.65 - Last mean reward per episode: 0.38
Num timesteps: 1380000
Best mean reward: 0.65 - Last mean reward per episode: 0.46
Num timesteps: 1381200
Best mean reward: 0.65 - Last mean reward per episode: 0.53
Num timesteps: 1382400
Best mean reward: 0.65 - Last mean reward per episode: 0.51
Num timesteps: 1383600
Best mean reward: 0.65 - Last mean reward per episode: 0.52
Num timesteps: 1384800
Best mean reward: 0.65 - Last mean reward per episode: 0.51
Num timesteps: 1386000
Best mean reward: 0.65 - Last mean reward per episode: 0.47
Num timesteps: 1387200
Best mean reward: 0.65 - Last mean reward per episode: 0.48
Num timesteps: 1388400
Best mean reward: 0.65 - Last mean reward per episode: 0.58
Num timesteps: 1389600
Best mean reward: 0.65 - Last mean reward per episode: 0.54
Num timesteps: 1390800
Best mean reward: 0.65 - Last mean reward per episode: 0.49
Num timesteps: 1392000
Best mean reward: 0.65 - Last mean reward per episode: 0.48
Num timesteps: 1393200
Best mean reward: 0.65 - Last mean reward per episode: 0.56
Num timesteps: 1394400
Best mean reward: 0.65 - Last mean reward per episode: 0.51
Num timesteps: 1395600
Best mean reward: 0.65 - Last mean reward per episode: 0.59
Num timesteps: 1396800
Best mean reward: 0.65 - Last mean reward per episode: 0.61
Num timesteps: 1398000
Best mean reward: 0.65 - Last mean reward per episode: 0.45
Num timesteps: 1399200
Best mean reward: 0.65 - Last mean reward per episode: 0.53
Num timesteps: 1400400
Best mean reward: 0.65 - Last mean reward per episode: 0.50
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 7.21       |
|    ep_rew_mean          | 0.44099998 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 57         |
|    time_elapsed         | 447869     |
|    total_timesteps      | 1400832    |
| train/                  |            |
|    approx_kl            | 2.4292772  |
|    clip_fraction        | 0.832      |
|    clip_range           | 0.2        |
|    entropy_loss         | 2.54       |
|    explained_variance   | 0.0799     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0292    |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0286    |
|    std                  | 0.0999     |
|    value_loss           | 0.171      |
----------------------------------------
Num timesteps: 1401600
Best mean reward: 0.65 - Last mean reward per episode: 0.51
Num timesteps: 1402800
Best mean reward: 0.65 - Last mean reward per episode: 0.57
Num timesteps: 1404000
Best mean reward: 0.65 - Last mean reward per episode: 0.42
Num timesteps: 1405200
Best mean reward: 0.65 - Last mean reward per episode: 0.51
Num timesteps: 1406400
Best mean reward: 0.65 - Last mean reward per episode: 0.51
Num timesteps: 1407600
Best mean reward: 0.65 - Last mean reward per episode: 0.52
Num timesteps: 1408800
Best mean reward: 0.65 - Last mean reward per episode: 0.52
Num timesteps: 1410000
Best mean reward: 0.65 - Last mean reward per episode: 0.53
Num timesteps: 1411200
Best mean reward: 0.65 - Last mean reward per episode: 0.55
Num timesteps: 1412400
Best mean reward: 0.65 - Last mean reward per episode: 0.48
Num timesteps: 1413600
Best mean reward: 0.65 - Last mean reward per episode: 0.52
Num timesteps: 1414800
Best mean reward: 0.65 - Last mean reward per episode: 0.49
Num timesteps: 1416000
Best mean reward: 0.65 - Last mean reward per episode: 0.52
Num timesteps: 1417200
Best mean reward: 0.65 - Last mean reward per episode: 0.45
Num timesteps: 1418400
Best mean reward: 0.65 - Last mean reward per episode: 0.62
Num timesteps: 1419600
Best mean reward: 0.65 - Last mean reward per episode: 0.57
Num timesteps: 1420800
Best mean reward: 0.65 - Last mean reward per episode: 0.43
Num timesteps: 1422000
Best mean reward: 0.65 - Last mean reward per episode: 0.44
Num timesteps: 1423200
Best mean reward: 0.65 - Last mean reward per episode: 0.53
Num timesteps: 1424400
Best mean reward: 0.65 - Last mean reward per episode: 0.49
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 7.46     |
|    ep_rew_mean          | 0.52     |
| time/                   |          |
|    fps                  | 3        |
|    iterations           | 58       |
|    time_elapsed         | 455721   |
|    total_timesteps      | 1425408  |
| train/                  |          |
|    approx_kl            | 2.053873 |
|    clip_fraction        | 0.836    |
|    clip_range           | 0.2      |
|    entropy_loss         | 2.67     |
|    explained_variance   | 0.102    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0301  |
|    n_updates            | 570      |
|    policy_gradient_loss | -0.0203  |
|    std                  | 0.0955   |
|    value_loss           | 0.165    |
--------------------------------------
Num timesteps: 1425600
Best mean reward: 0.65 - Last mean reward per episode: 0.49
Num timesteps: 1426800
Best mean reward: 0.65 - Last mean reward per episode: 0.53
Num timesteps: 1428000
Best mean reward: 0.65 - Last mean reward per episode: 0.54
Num timesteps: 1429200
Best mean reward: 0.65 - Last mean reward per episode: 0.55
Num timesteps: 1430400
Best mean reward: 0.65 - Last mean reward per episode: 0.49
Num timesteps: 1431600
Best mean reward: 0.65 - Last mean reward per episode: 0.58
Num timesteps: 1432800
Best mean reward: 0.65 - Last mean reward per episode: 0.54
Num timesteps: 1434000
Best mean reward: 0.65 - Last mean reward per episode: 0.60
Num timesteps: 1435200
Best mean reward: 0.65 - Last mean reward per episode: 0.47
Num timesteps: 1436400
Best mean reward: 0.65 - Last mean reward per episode: 0.57
Num timesteps: 1437600
Best mean reward: 0.65 - Last mean reward per episode: 0.54
Num timesteps: 1438800
Best mean reward: 0.65 - Last mean reward per episode: 0.69
Saving new best model to models/train_stack2/best_model
Num timesteps: 1440000
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1441200
Best mean reward: 0.69 - Last mean reward per episode: 0.48
Num timesteps: 1442400
Best mean reward: 0.69 - Last mean reward per episode: 0.50
Num timesteps: 1443600
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1444800
Best mean reward: 0.69 - Last mean reward per episode: 0.42
Num timesteps: 1446000
Best mean reward: 0.69 - Last mean reward per episode: 0.44
Num timesteps: 1447200
Best mean reward: 0.69 - Last mean reward per episode: 0.61
Num timesteps: 1448400
Best mean reward: 0.69 - Last mean reward per episode: 0.49
Num timesteps: 1449600
Best mean reward: 0.69 - Last mean reward per episode: 0.50
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 7.38       |
|    ep_rew_mean          | 0.48300004 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 59         |
|    time_elapsed         | 463605     |
|    total_timesteps      | 1449984    |
| train/                  |            |
|    approx_kl            | 2.6451724  |
|    clip_fraction        | 0.843      |
|    clip_range           | 0.2        |
|    entropy_loss         | 2.78       |
|    explained_variance   | 0.0802     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0361    |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.00474   |
|    std                  | 0.0926     |
|    value_loss           | 0.166      |
----------------------------------------
Num timesteps: 1450800
Best mean reward: 0.69 - Last mean reward per episode: 0.57
Num timesteps: 1452000
Best mean reward: 0.69 - Last mean reward per episode: 0.63
Num timesteps: 1453200
Best mean reward: 0.69 - Last mean reward per episode: 0.50
Num timesteps: 1454400
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1455600
Best mean reward: 0.69 - Last mean reward per episode: 0.62
Num timesteps: 1456800
Best mean reward: 0.69 - Last mean reward per episode: 0.43
Num timesteps: 1458000
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1459200
Best mean reward: 0.69 - Last mean reward per episode: 0.53
Num timesteps: 1460400
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1461600
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1462800
Best mean reward: 0.69 - Last mean reward per episode: 0.57
Num timesteps: 1464000
Best mean reward: 0.69 - Last mean reward per episode: 0.61
Num timesteps: 1465200
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1466400
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1467600
Best mean reward: 0.69 - Last mean reward per episode: 0.50
Num timesteps: 1468800
Best mean reward: 0.69 - Last mean reward per episode: 0.58
Num timesteps: 1470000
Best mean reward: 0.69 - Last mean reward per episode: 0.53
Num timesteps: 1471200
Best mean reward: 0.69 - Last mean reward per episode: 0.48
Num timesteps: 1472400
Best mean reward: 0.69 - Last mean reward per episode: 0.53
Num timesteps: 1473600
Best mean reward: 0.69 - Last mean reward per episode: 0.59
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 7.64       |
|    ep_rew_mean          | 0.55500007 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 60         |
|    time_elapsed         | 471502     |
|    total_timesteps      | 1474560    |
| train/                  |            |
|    approx_kl            | 2.1749647  |
|    clip_fraction        | 0.846      |
|    clip_range           | 0.2        |
|    entropy_loss         | 2.87       |
|    explained_variance   | 0.0953     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0182    |
|    n_updates            | 590        |
|    policy_gradient_loss | 0.00527    |
|    std                  | 0.0895     |
|    value_loss           | 0.162      |
----------------------------------------
Num timesteps: 1474800
Best mean reward: 0.69 - Last mean reward per episode: 0.59
Num timesteps: 1476000
Best mean reward: 0.69 - Last mean reward per episode: 0.48
Num timesteps: 1477200
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1478400
Best mean reward: 0.69 - Last mean reward per episode: 0.57
Num timesteps: 1479600
Best mean reward: 0.69 - Last mean reward per episode: 0.63
Num timesteps: 1480800
Best mean reward: 0.69 - Last mean reward per episode: 0.54
Num timesteps: 1482000
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1483200
Best mean reward: 0.69 - Last mean reward per episode: 0.49
Num timesteps: 1484400
Best mean reward: 0.69 - Last mean reward per episode: 0.48
Num timesteps: 1485600
Best mean reward: 0.69 - Last mean reward per episode: 0.60
Num timesteps: 1486800
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1488000
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1489200
Best mean reward: 0.69 - Last mean reward per episode: 0.41
Num timesteps: 1490400
Best mean reward: 0.69 - Last mean reward per episode: 0.54
Num timesteps: 1491600
Best mean reward: 0.69 - Last mean reward per episode: 0.57
Num timesteps: 1492800
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1494000
Best mean reward: 0.69 - Last mean reward per episode: 0.50
Num timesteps: 1495200
Best mean reward: 0.69 - Last mean reward per episode: 0.54
Num timesteps: 1496400
Best mean reward: 0.69 - Last mean reward per episode: 0.58
Num timesteps: 1497600
Best mean reward: 0.69 - Last mean reward per episode: 0.48
Num timesteps: 1498800
Best mean reward: 0.69 - Last mean reward per episode: 0.56
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.96       |
|    ep_rew_mean          | 0.54200006 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 61         |
|    time_elapsed         | 479386     |
|    total_timesteps      | 1499136    |
| train/                  |            |
|    approx_kl            | 2.077883   |
|    clip_fraction        | 0.846      |
|    clip_range           | 0.2        |
|    entropy_loss         | 2.97       |
|    explained_variance   | 0.0617     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00605   |
|    n_updates            | 600        |
|    policy_gradient_loss | 0.00454    |
|    std                  | 0.0867     |
|    value_loss           | 0.165      |
----------------------------------------
Num timesteps: 1500000
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1501200
Best mean reward: 0.69 - Last mean reward per episode: 0.54
Num timesteps: 1502400
Best mean reward: 0.69 - Last mean reward per episode: 0.57
Num timesteps: 1503600
Best mean reward: 0.69 - Last mean reward per episode: 0.46
Num timesteps: 1504800
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1506000
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1507200
Best mean reward: 0.69 - Last mean reward per episode: 0.54
Num timesteps: 1508400
Best mean reward: 0.69 - Last mean reward per episode: 0.59
Num timesteps: 1509600
Best mean reward: 0.69 - Last mean reward per episode: 0.61
Num timesteps: 1510800
Best mean reward: 0.69 - Last mean reward per episode: 0.49
Num timesteps: 1512000
Best mean reward: 0.69 - Last mean reward per episode: 0.59
Num timesteps: 1513200
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1514400
Best mean reward: 0.69 - Last mean reward per episode: 0.50
Num timesteps: 1515600
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1516800
Best mean reward: 0.69 - Last mean reward per episode: 0.50
Num timesteps: 1518000
Best mean reward: 0.69 - Last mean reward per episode: 0.54
Num timesteps: 1519200
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1520400
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1521600
Best mean reward: 0.69 - Last mean reward per episode: 0.54
Num timesteps: 1522800
Best mean reward: 0.69 - Last mean reward per episode: 0.55
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 6.52      |
|    ep_rew_mean          | 0.503     |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 62        |
|    time_elapsed         | 487281    |
|    total_timesteps      | 1523712   |
| train/                  |           |
|    approx_kl            | 2.5863545 |
|    clip_fraction        | 0.839     |
|    clip_range           | 0.2       |
|    entropy_loss         | 3.08      |
|    explained_variance   | 0.0867    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0389   |
|    n_updates            | 610       |
|    policy_gradient_loss | -0.00735  |
|    std                  | 0.0836    |
|    value_loss           | 0.163     |
---------------------------------------
Num timesteps: 1524000
Best mean reward: 0.69 - Last mean reward per episode: 0.47
Num timesteps: 1525200
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1526400
Best mean reward: 0.69 - Last mean reward per episode: 0.53
Num timesteps: 1527600
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1528800
Best mean reward: 0.69 - Last mean reward per episode: 0.49
Num timesteps: 1530000
Best mean reward: 0.69 - Last mean reward per episode: 0.42
Num timesteps: 1531200
Best mean reward: 0.69 - Last mean reward per episode: 0.50
Num timesteps: 1532400
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1533600
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1534800
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1536000
Best mean reward: 0.69 - Last mean reward per episode: 0.61
Num timesteps: 1537200
Best mean reward: 0.69 - Last mean reward per episode: 0.49
Num timesteps: 1538400
Best mean reward: 0.69 - Last mean reward per episode: 0.64
Num timesteps: 1539600
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1540800
Best mean reward: 0.69 - Last mean reward per episode: 0.64
Num timesteps: 1542000
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1543200
Best mean reward: 0.69 - Last mean reward per episode: 0.53
Num timesteps: 1544400
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1545600
Best mean reward: 0.69 - Last mean reward per episode: 0.53
Num timesteps: 1546800
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1548000
Best mean reward: 0.69 - Last mean reward per episode: 0.58
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 6.76      |
|    ep_rew_mean          | 0.574     |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 63        |
|    time_elapsed         | 495187    |
|    total_timesteps      | 1548288   |
| train/                  |           |
|    approx_kl            | 2.2542288 |
|    clip_fraction        | 0.849     |
|    clip_range           | 0.2       |
|    entropy_loss         | 3.17      |
|    explained_variance   | 0.0818    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00976  |
|    n_updates            | 620       |
|    policy_gradient_loss | 0.00814   |
|    std                  | 0.0809    |
|    value_loss           | 0.163     |
---------------------------------------
Num timesteps: 1549200
Best mean reward: 0.69 - Last mean reward per episode: 0.60
Num timesteps: 1550400
Best mean reward: 0.69 - Last mean reward per episode: 0.61
Num timesteps: 1551600
Best mean reward: 0.69 - Last mean reward per episode: 0.59
Num timesteps: 1552800
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1554000
Best mean reward: 0.69 - Last mean reward per episode: 0.49
Num timesteps: 1555200
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1556400
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1557600
Best mean reward: 0.69 - Last mean reward per episode: 0.50
Num timesteps: 1558800
Best mean reward: 0.69 - Last mean reward per episode: 0.62
Num timesteps: 1560000
Best mean reward: 0.69 - Last mean reward per episode: 0.58
Num timesteps: 1561200
Best mean reward: 0.69 - Last mean reward per episode: 0.58
Num timesteps: 1562400
Best mean reward: 0.69 - Last mean reward per episode: 0.44
Num timesteps: 1563600
Best mean reward: 0.69 - Last mean reward per episode: 0.59
Num timesteps: 1564800
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1566000
Best mean reward: 0.69 - Last mean reward per episode: 0.61
Num timesteps: 1567200
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1568400
Best mean reward: 0.69 - Last mean reward per episode: 0.58
Num timesteps: 1569600
Best mean reward: 0.69 - Last mean reward per episode: 0.59
Num timesteps: 1570800
Best mean reward: 0.69 - Last mean reward per episode: 0.60
Num timesteps: 1572000
Best mean reward: 0.69 - Last mean reward per episode: 0.51
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 6.98      |
|    ep_rew_mean          | 0.609     |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 64        |
|    time_elapsed         | 503056    |
|    total_timesteps      | 1572864   |
| train/                  |           |
|    approx_kl            | 2.5963733 |
|    clip_fraction        | 0.852     |
|    clip_range           | 0.2       |
|    entropy_loss         | 3.27      |
|    explained_variance   | 0.0897    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0137   |
|    n_updates            | 630       |
|    policy_gradient_loss | 0.0179    |
|    std                  | 0.079     |
|    value_loss           | 0.165     |
---------------------------------------
Num timesteps: 1573200
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1574400
Best mean reward: 0.69 - Last mean reward per episode: 0.65
Num timesteps: 1575600
Best mean reward: 0.69 - Last mean reward per episode: 0.47
Num timesteps: 1576800
Best mean reward: 0.69 - Last mean reward per episode: 0.62
Num timesteps: 1578000
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1579200
Best mean reward: 0.69 - Last mean reward per episode: 0.46
Num timesteps: 1580400
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1581600
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1582800
Best mean reward: 0.69 - Last mean reward per episode: 0.60
Num timesteps: 1584000
Best mean reward: 0.69 - Last mean reward per episode: 0.49
Num timesteps: 1585200
Best mean reward: 0.69 - Last mean reward per episode: 0.58
Num timesteps: 1586400
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1587600
Best mean reward: 0.69 - Last mean reward per episode: 0.53
Num timesteps: 1588800
Best mean reward: 0.69 - Last mean reward per episode: 0.59
Num timesteps: 1590000
Best mean reward: 0.69 - Last mean reward per episode: 0.66
Num timesteps: 1591200
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1592400
Best mean reward: 0.69 - Last mean reward per episode: 0.47
Num timesteps: 1593600
Best mean reward: 0.69 - Last mean reward per episode: 0.43
Num timesteps: 1594800
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1596000
Best mean reward: 0.69 - Last mean reward per episode: 0.53
Num timesteps: 1597200
Best mean reward: 0.69 - Last mean reward per episode: 0.46
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 7.01       |
|    ep_rew_mean          | 0.49600005 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 65         |
|    time_elapsed         | 510948     |
|    total_timesteps      | 1597440    |
| train/                  |            |
|    approx_kl            | 2.7101402  |
|    clip_fraction        | 0.857      |
|    clip_range           | 0.2        |
|    entropy_loss         | 3.33       |
|    explained_variance   | 0.0948     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00261   |
|    n_updates            | 640        |
|    policy_gradient_loss | 0.031      |
|    std                  | 0.0771     |
|    value_loss           | 0.165      |
----------------------------------------
Num timesteps: 1598400
Best mean reward: 0.69 - Last mean reward per episode: 0.63
Num timesteps: 1599600
Best mean reward: 0.69 - Last mean reward per episode: 0.43
Num timesteps: 1600800
Best mean reward: 0.69 - Last mean reward per episode: 0.49
Num timesteps: 1602000
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1603200
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1604400
Best mean reward: 0.69 - Last mean reward per episode: 0.58
Num timesteps: 1605600
Best mean reward: 0.69 - Last mean reward per episode: 0.57
Num timesteps: 1606800
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1608000
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1609200
Best mean reward: 0.69 - Last mean reward per episode: 0.50
Num timesteps: 1610400
Best mean reward: 0.69 - Last mean reward per episode: 0.61
Num timesteps: 1611600
Best mean reward: 0.69 - Last mean reward per episode: 0.62
Num timesteps: 1612800
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1614000
Best mean reward: 0.69 - Last mean reward per episode: 0.45
Num timesteps: 1615200
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1616400
Best mean reward: 0.69 - Last mean reward per episode: 0.59
Num timesteps: 1617600
Best mean reward: 0.69 - Last mean reward per episode: 0.48
Num timesteps: 1618800
Best mean reward: 0.69 - Last mean reward per episode: 0.50
Num timesteps: 1620000
Best mean reward: 0.69 - Last mean reward per episode: 0.53
Num timesteps: 1621200
Best mean reward: 0.69 - Last mean reward per episode: 0.53
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 6.54      |
|    ep_rew_mean          | 0.5590001 |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 66        |
|    time_elapsed         | 518851    |
|    total_timesteps      | 1622016   |
| train/                  |           |
|    approx_kl            | 2.2905781 |
|    clip_fraction        | 0.862     |
|    clip_range           | 0.2       |
|    entropy_loss         | 3.39      |
|    explained_variance   | 0.0905    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00304  |
|    n_updates            | 650       |
|    policy_gradient_loss | 0.0517    |
|    std                  | 0.076     |
|    value_loss           | 0.167     |
---------------------------------------
Num timesteps: 1622400
Best mean reward: 0.69 - Last mean reward per episode: 0.57
Num timesteps: 1623600
Best mean reward: 0.69 - Last mean reward per episode: 0.58
Num timesteps: 1624800
Best mean reward: 0.69 - Last mean reward per episode: 0.59
Num timesteps: 1626000
Best mean reward: 0.69 - Last mean reward per episode: 0.58
Num timesteps: 1627200
Best mean reward: 0.69 - Last mean reward per episode: 0.48
Num timesteps: 1628400
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1629600
Best mean reward: 0.69 - Last mean reward per episode: 0.57
Num timesteps: 1630800
Best mean reward: 0.69 - Last mean reward per episode: 0.49
Num timesteps: 1632000
Best mean reward: 0.69 - Last mean reward per episode: 0.65
Num timesteps: 1633200
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1634400
Best mean reward: 0.69 - Last mean reward per episode: 0.53
Num timesteps: 1635600
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1636800
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1638000
Best mean reward: 0.69 - Last mean reward per episode: 0.50
Num timesteps: 1639200
Best mean reward: 0.69 - Last mean reward per episode: 0.49
Num timesteps: 1640400
Best mean reward: 0.69 - Last mean reward per episode: 0.57
Num timesteps: 1641600
Best mean reward: 0.69 - Last mean reward per episode: 0.63
Num timesteps: 1642800
Best mean reward: 0.69 - Last mean reward per episode: 0.61
Num timesteps: 1644000
Best mean reward: 0.69 - Last mean reward per episode: 0.53
Num timesteps: 1645200
Best mean reward: 0.69 - Last mean reward per episode: 0.61
Num timesteps: 1646400
Best mean reward: 0.69 - Last mean reward per episode: 0.57
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 7.09      |
|    ep_rew_mean          | 0.6050001 |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 67        |
|    time_elapsed         | 526745    |
|    total_timesteps      | 1646592   |
| train/                  |           |
|    approx_kl            | 2.3102677 |
|    clip_fraction        | 0.851     |
|    clip_range           | 0.2       |
|    entropy_loss         | 3.45      |
|    explained_variance   | 0.0859    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0198   |
|    n_updates            | 660       |
|    policy_gradient_loss | 0.0167    |
|    std                  | 0.0743    |
|    value_loss           | 0.171     |
---------------------------------------
Num timesteps: 1647600
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1648800
Best mean reward: 0.69 - Last mean reward per episode: 0.57
Num timesteps: 1650000
Best mean reward: 0.69 - Last mean reward per episode: 0.54
Num timesteps: 1651200
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1652400
Best mean reward: 0.69 - Last mean reward per episode: 0.57
Num timesteps: 1653600
Best mean reward: 0.69 - Last mean reward per episode: 0.46
Num timesteps: 1654800
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1656000
Best mean reward: 0.69 - Last mean reward per episode: 0.59
Num timesteps: 1657200
Best mean reward: 0.69 - Last mean reward per episode: 0.59
Num timesteps: 1658400
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1659600
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1660800
Best mean reward: 0.69 - Last mean reward per episode: 0.49
Num timesteps: 1662000
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1663200
Best mean reward: 0.69 - Last mean reward per episode: 0.54
Num timesteps: 1664400
Best mean reward: 0.69 - Last mean reward per episode: 0.54
Num timesteps: 1665600
Best mean reward: 0.69 - Last mean reward per episode: 0.61
Num timesteps: 1666800
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1668000
Best mean reward: 0.69 - Last mean reward per episode: 0.50
Num timesteps: 1669200
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1670400
Best mean reward: 0.69 - Last mean reward per episode: 0.54
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 6.45     |
|    ep_rew_mean          | 0.469    |
| time/                   |          |
|    fps                  | 3        |
|    iterations           | 68       |
|    time_elapsed         | 534653   |
|    total_timesteps      | 1671168  |
| train/                  |          |
|    approx_kl            | 2.274252 |
|    clip_fraction        | 0.856    |
|    clip_range           | 0.2      |
|    entropy_loss         | 3.51     |
|    explained_variance   | 0.1      |
|    learning_rate        | 0.0003   |
|    loss                 | -0.00425 |
|    n_updates            | 670      |
|    policy_gradient_loss | 0.0361   |
|    std                  | 0.0729   |
|    value_loss           | 0.161    |
--------------------------------------
Num timesteps: 1671600
Best mean reward: 0.69 - Last mean reward per episode: 0.49
Num timesteps: 1672800
Best mean reward: 0.69 - Last mean reward per episode: 0.50
Num timesteps: 1674000
Best mean reward: 0.69 - Last mean reward per episode: 0.59
Num timesteps: 1675200
Best mean reward: 0.69 - Last mean reward per episode: 0.54
Num timesteps: 1676400
Best mean reward: 0.69 - Last mean reward per episode: 0.63
Num timesteps: 1677600
Best mean reward: 0.69 - Last mean reward per episode: 0.57
Num timesteps: 1678800
Best mean reward: 0.69 - Last mean reward per episode: 0.62
Num timesteps: 1680000
Best mean reward: 0.69 - Last mean reward per episode: 0.63
Num timesteps: 1681200
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1682400
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1683600
Best mean reward: 0.69 - Last mean reward per episode: 0.57
Num timesteps: 1684800
Best mean reward: 0.69 - Last mean reward per episode: 0.60
Num timesteps: 1686000
Best mean reward: 0.69 - Last mean reward per episode: 0.58
Num timesteps: 1687200
Best mean reward: 0.69 - Last mean reward per episode: 0.63
Num timesteps: 1688400
Best mean reward: 0.69 - Last mean reward per episode: 0.62
Num timesteps: 1689600
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1690800
Best mean reward: 0.69 - Last mean reward per episode: 0.53
Num timesteps: 1692000
Best mean reward: 0.69 - Last mean reward per episode: 0.60
Num timesteps: 1693200
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1694400
Best mean reward: 0.69 - Last mean reward per episode: 0.61
Num timesteps: 1695600
Best mean reward: 0.69 - Last mean reward per episode: 0.62
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 6.49      |
|    ep_rew_mean          | 0.73      |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 69        |
|    time_elapsed         | 542555    |
|    total_timesteps      | 1695744   |
| train/                  |           |
|    approx_kl            | 2.2151663 |
|    clip_fraction        | 0.854     |
|    clip_range           | 0.2       |
|    entropy_loss         | 3.57      |
|    explained_variance   | 0.0753    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00447  |
|    n_updates            | 680       |
|    policy_gradient_loss | 0.032     |
|    std                  | 0.0715    |
|    value_loss           | 0.17      |
---------------------------------------
Num timesteps: 1696800
Best mean reward: 0.69 - Last mean reward per episode: 0.66
Num timesteps: 1698000
Best mean reward: 0.69 - Last mean reward per episode: 0.61
Num timesteps: 1699200
Best mean reward: 0.69 - Last mean reward per episode: 0.53
Num timesteps: 1700400
Best mean reward: 0.69 - Last mean reward per episode: 0.54
Num timesteps: 1701600
Best mean reward: 0.69 - Last mean reward per episode: 0.62
Num timesteps: 1702800
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1704000
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1705200
Best mean reward: 0.69 - Last mean reward per episode: 0.58
Num timesteps: 1706400
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1707600
Best mean reward: 0.69 - Last mean reward per episode: 0.49
Num timesteps: 1708800
Best mean reward: 0.69 - Last mean reward per episode: 0.42
Num timesteps: 1710000
Best mean reward: 0.69 - Last mean reward per episode: 0.48
Num timesteps: 1711200
Best mean reward: 0.69 - Last mean reward per episode: 0.61
Num timesteps: 1712400
Best mean reward: 0.69 - Last mean reward per episode: 0.52
Num timesteps: 1713600
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1714800
Best mean reward: 0.69 - Last mean reward per episode: 0.57
Num timesteps: 1716000
Best mean reward: 0.69 - Last mean reward per episode: 0.65
Num timesteps: 1717200
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1718400
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1719600
Best mean reward: 0.69 - Last mean reward per episode: 0.57
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.72       |
|    ep_rew_mean          | 0.59199995 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 70         |
|    time_elapsed         | 550461     |
|    total_timesteps      | 1720320    |
| train/                  |            |
|    approx_kl            | 2.4827774  |
|    clip_fraction        | 0.843      |
|    clip_range           | 0.2        |
|    entropy_loss         | 3.66       |
|    explained_variance   | 0.102      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0207    |
|    n_updates            | 690        |
|    policy_gradient_loss | 0.00398    |
|    std                  | 0.069      |
|    value_loss           | 0.16       |
----------------------------------------
Num timesteps: 1720800
Best mean reward: 0.69 - Last mean reward per episode: 0.61
Num timesteps: 1722000
Best mean reward: 0.69 - Last mean reward per episode: 0.51
Num timesteps: 1723200
Best mean reward: 0.69 - Last mean reward per episode: 0.62
Num timesteps: 1724400
Best mean reward: 0.69 - Last mean reward per episode: 0.61
Num timesteps: 1725600
Best mean reward: 0.69 - Last mean reward per episode: 0.66
Num timesteps: 1726800
Best mean reward: 0.69 - Last mean reward per episode: 0.64
Num timesteps: 1728000
Best mean reward: 0.69 - Last mean reward per episode: 0.56
Num timesteps: 1729200
Best mean reward: 0.69 - Last mean reward per episode: 0.44
Num timesteps: 1730400
Best mean reward: 0.69 - Last mean reward per episode: 0.55
Num timesteps: 1731600
Best mean reward: 0.69 - Last mean reward per episode: 0.49
Num timesteps: 1732800
Best mean reward: 0.69 - Last mean reward per episode: 0.46
Num timesteps: 1734000
Best mean reward: 0.69 - Last mean reward per episode: 0.58
Num timesteps: 1735200
Best mean reward: 0.69 - Last mean reward per episode: 0.65
Num timesteps: 1736400
Best mean reward: 0.69 - Last mean reward per episode: 0.63
Num timesteps: 1737600
Best mean reward: 0.69 - Last mean reward per episode: 0.60
Num timesteps: 1738800
Best mean reward: 0.69 - Last mean reward per episode: 0.64
Num timesteps: 1740000
Best mean reward: 0.69 - Last mean reward per episode: 0.70
Saving new best model to models/train_stack2/best_model
Num timesteps: 1741200
Best mean reward: 0.70 - Last mean reward per episode: 0.65
Num timesteps: 1742400
Best mean reward: 0.70 - Last mean reward per episode: 0.65
Num timesteps: 1743600
Best mean reward: 0.70 - Last mean reward per episode: 0.55
Num timesteps: 1744800
Best mean reward: 0.70 - Last mean reward per episode: 0.46
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 7.33     |
|    ep_rew_mean          | 0.43     |
| time/                   |          |
|    fps                  | 3        |
|    iterations           | 71       |
|    time_elapsed         | 558340   |
|    total_timesteps      | 1744896  |
| train/                  |          |
|    approx_kl            | 2.344789 |
|    clip_fraction        | 0.86     |
|    clip_range           | 0.2      |
|    entropy_loss         | 3.72     |
|    explained_variance   | 0.103    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.00738 |
|    n_updates            | 700      |
|    policy_gradient_loss | 0.0377   |
|    std                  | 0.068    |
|    value_loss           | 0.16     |
--------------------------------------
Num timesteps: 1746000
Best mean reward: 0.70 - Last mean reward per episode: 0.66
Num timesteps: 1747200
Best mean reward: 0.70 - Last mean reward per episode: 0.63
Num timesteps: 1748400
Best mean reward: 0.70 - Last mean reward per episode: 0.55
Num timesteps: 1749600
Best mean reward: 0.70 - Last mean reward per episode: 0.58
Num timesteps: 1750800
Best mean reward: 0.70 - Last mean reward per episode: 0.65
Num timesteps: 1752000
Best mean reward: 0.70 - Last mean reward per episode: 0.61
Num timesteps: 1753200
Best mean reward: 0.70 - Last mean reward per episode: 0.53
Num timesteps: 1754400
Best mean reward: 0.70 - Last mean reward per episode: 0.54
Num timesteps: 1755600
Best mean reward: 0.70 - Last mean reward per episode: 0.61
Num timesteps: 1756800
Best mean reward: 0.70 - Last mean reward per episode: 0.53
Num timesteps: 1758000
Best mean reward: 0.70 - Last mean reward per episode: 0.54
Num timesteps: 1759200
Best mean reward: 0.70 - Last mean reward per episode: 0.67
Num timesteps: 1760400
Best mean reward: 0.70 - Last mean reward per episode: 0.53
Num timesteps: 1761600
Best mean reward: 0.70 - Last mean reward per episode: 0.56
Num timesteps: 1762800
Best mean reward: 0.70 - Last mean reward per episode: 0.56
Num timesteps: 1764000
Best mean reward: 0.70 - Last mean reward per episode: 0.39
Num timesteps: 1765200
Best mean reward: 0.70 - Last mean reward per episode: 0.55
Num timesteps: 1766400
Best mean reward: 0.70 - Last mean reward per episode: 0.54
Num timesteps: 1767600
Best mean reward: 0.70 - Last mean reward per episode: 0.58
Num timesteps: 1768800
Best mean reward: 0.70 - Last mean reward per episode: 0.44
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 7.88      |
|    ep_rew_mean          | 0.61      |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 72        |
|    time_elapsed         | 566264    |
|    total_timesteps      | 1769472   |
| train/                  |           |
|    approx_kl            | 2.4954622 |
|    clip_fraction        | 0.862     |
|    clip_range           | 0.2       |
|    entropy_loss         | 3.77      |
|    explained_variance   | 0.105     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0149    |
|    n_updates            | 710       |
|    policy_gradient_loss | 0.0343    |
|    std                  | 0.0667    |
|    value_loss           | 0.157     |
---------------------------------------
Num timesteps: 1770000
Best mean reward: 0.70 - Last mean reward per episode: 0.68
Num timesteps: 1771200
Best mean reward: 0.70 - Last mean reward per episode: 0.55
Num timesteps: 1772400
Best mean reward: 0.70 - Last mean reward per episode: 0.58
Num timesteps: 1773600
Best mean reward: 0.70 - Last mean reward per episode: 0.67
Num timesteps: 1774800
Best mean reward: 0.70 - Last mean reward per episode: 0.51
Num timesteps: 1776000
Best mean reward: 0.70 - Last mean reward per episode: 0.57
Num timesteps: 1777200
Best mean reward: 0.70 - Last mean reward per episode: 0.60
Num timesteps: 1778400
Best mean reward: 0.70 - Last mean reward per episode: 0.58
Num timesteps: 1779600
Best mean reward: 0.70 - Last mean reward per episode: 0.60
Num timesteps: 1780800
Best mean reward: 0.70 - Last mean reward per episode: 0.59
Num timesteps: 1782000
Best mean reward: 0.70 - Last mean reward per episode: 0.53
Num timesteps: 1783200
Best mean reward: 0.70 - Last mean reward per episode: 0.54
Num timesteps: 1784400
Best mean reward: 0.70 - Last mean reward per episode: 0.60
Num timesteps: 1785600
Best mean reward: 0.70 - Last mean reward per episode: 0.58
Num timesteps: 1786800
Best mean reward: 0.70 - Last mean reward per episode: 0.60
Num timesteps: 1788000
Best mean reward: 0.70 - Last mean reward per episode: 0.59
Num timesteps: 1789200
Best mean reward: 0.70 - Last mean reward per episode: 0.64
Num timesteps: 1790400
Best mean reward: 0.70 - Last mean reward per episode: 0.53
Num timesteps: 1791600
Best mean reward: 0.70 - Last mean reward per episode: 0.45
Num timesteps: 1792800
Best mean reward: 0.70 - Last mean reward per episode: 0.64
Num timesteps: 1794000
Best mean reward: 0.70 - Last mean reward per episode: 0.60
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 7.22      |
|    ep_rew_mean          | 0.5810001 |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 73        |
|    time_elapsed         | 574153    |
|    total_timesteps      | 1794048   |
| train/                  |           |
|    approx_kl            | 2.4564197 |
|    clip_fraction        | 0.872     |
|    clip_range           | 0.2       |
|    entropy_loss         | 3.8       |
|    explained_variance   | 0.0883    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0247    |
|    n_updates            | 720       |
|    policy_gradient_loss | 0.0788    |
|    std                  | 0.0666    |
|    value_loss           | 0.157     |
---------------------------------------
Num timesteps: 1795200
Best mean reward: 0.70 - Last mean reward per episode: 0.63
Num timesteps: 1796400
Best mean reward: 0.70 - Last mean reward per episode: 0.60
Num timesteps: 1797600
Best mean reward: 0.70 - Last mean reward per episode: 0.57
Num timesteps: 1798800
Best mean reward: 0.70 - Last mean reward per episode: 0.63
Num timesteps: 1800000
Best mean reward: 0.70 - Last mean reward per episode: 0.60
Num timesteps: 1801200
Best mean reward: 0.70 - Last mean reward per episode: 0.60
Num timesteps: 1802400
Best mean reward: 0.70 - Last mean reward per episode: 0.59
Num timesteps: 1803600
Best mean reward: 0.70 - Last mean reward per episode: 0.63
Num timesteps: 1804800
Best mean reward: 0.70 - Last mean reward per episode: 0.55
Num timesteps: 1806000
Best mean reward: 0.70 - Last mean reward per episode: 0.68
Num timesteps: 1807200
Best mean reward: 0.70 - Last mean reward per episode: 0.56
Num timesteps: 1808400
Best mean reward: 0.70 - Last mean reward per episode: 0.54
Num timesteps: 1809600
Best mean reward: 0.70 - Last mean reward per episode: 0.65
Num timesteps: 1810800
Best mean reward: 0.70 - Last mean reward per episode: 0.60
Num timesteps: 1812000
Best mean reward: 0.70 - Last mean reward per episode: 0.62
Num timesteps: 1813200
Best mean reward: 0.70 - Last mean reward per episode: 0.63
Num timesteps: 1814400
Best mean reward: 0.70 - Last mean reward per episode: 0.50
Num timesteps: 1815600
Best mean reward: 0.70 - Last mean reward per episode: 0.70
Saving new best model to models/train_stack2/best_model
Num timesteps: 1816800
Best mean reward: 0.70 - Last mean reward per episode: 0.56
Num timesteps: 1818000
Best mean reward: 0.70 - Last mean reward per episode: 0.51
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 7.56     |
|    ep_rew_mean          | 0.571    |
| time/                   |          |
|    fps                  | 3        |
|    iterations           | 74       |
|    time_elapsed         | 582037   |
|    total_timesteps      | 1818624  |
| train/                  |          |
|    approx_kl            | 2.513292 |
|    clip_fraction        | 0.862    |
|    clip_range           | 0.2      |
|    entropy_loss         | 3.82     |
|    explained_variance   | 0.104    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0149  |
|    n_updates            | 730      |
|    policy_gradient_loss | 0.0434   |
|    std                  | 0.0658   |
|    value_loss           | 0.152    |
--------------------------------------
Num timesteps: 1819200
Best mean reward: 0.70 - Last mean reward per episode: 0.58
Num timesteps: 1820400
Best mean reward: 0.70 - Last mean reward per episode: 0.55
Num timesteps: 1821600
Best mean reward: 0.70 - Last mean reward per episode: 0.57
Num timesteps: 1822800
Best mean reward: 0.70 - Last mean reward per episode: 0.54
Num timesteps: 1824000
Best mean reward: 0.70 - Last mean reward per episode: 0.59
Num timesteps: 1825200
Best mean reward: 0.70 - Last mean reward per episode: 0.65
Num timesteps: 1826400
Best mean reward: 0.70 - Last mean reward per episode: 0.52
Num timesteps: 1827600
Best mean reward: 0.70 - Last mean reward per episode: 0.67
Num timesteps: 1828800
Best mean reward: 0.70 - Last mean reward per episode: 0.54
Num timesteps: 1830000
Best mean reward: 0.70 - Last mean reward per episode: 0.56
Num timesteps: 1831200
Best mean reward: 0.70 - Last mean reward per episode: 0.54
Num timesteps: 1832400
Best mean reward: 0.70 - Last mean reward per episode: 0.58
Num timesteps: 1833600
Best mean reward: 0.70 - Last mean reward per episode: 0.59
Num timesteps: 1834800
Best mean reward: 0.70 - Last mean reward per episode: 0.62
Num timesteps: 1836000
Best mean reward: 0.70 - Last mean reward per episode: 0.57
Num timesteps: 1837200
Best mean reward: 0.70 - Last mean reward per episode: 0.61
Num timesteps: 1838400
Best mean reward: 0.70 - Last mean reward per episode: 0.56
Num timesteps: 1839600
Best mean reward: 0.70 - Last mean reward per episode: 0.60
Num timesteps: 1840800
Best mean reward: 0.70 - Last mean reward per episode: 0.55
Num timesteps: 1842000
Best mean reward: 0.70 - Last mean reward per episode: 0.51
Num timesteps: 1843200
Best mean reward: 0.70 - Last mean reward per episode: 0.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 7.59       |
|    ep_rew_mean          | 0.45100003 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 75         |
|    time_elapsed         | 589917     |
|    total_timesteps      | 1843200    |
| train/                  |            |
|    approx_kl            | 2.2358935  |
|    clip_fraction        | 0.857      |
|    clip_range           | 0.2        |
|    entropy_loss         | 3.88       |
|    explained_variance   | 0.116      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0108    |
|    n_updates            | 740        |
|    policy_gradient_loss | 0.0409     |
|    std                  | 0.0645     |
|    value_loss           | 0.153      |
----------------------------------------
Num timesteps: 1844400
Best mean reward: 0.70 - Last mean reward per episode: 0.62
Num timesteps: 1845600
Best mean reward: 0.70 - Last mean reward per episode: 0.50
Num timesteps: 1846800
Best mean reward: 0.70 - Last mean reward per episode: 0.65
Num timesteps: 1848000
Best mean reward: 0.70 - Last mean reward per episode: 0.58
Num timesteps: 1849200
Best mean reward: 0.70 - Last mean reward per episode: 0.62
Num timesteps: 1850400
Best mean reward: 0.70 - Last mean reward per episode: 0.60
Num timesteps: 1851600
Best mean reward: 0.70 - Last mean reward per episode: 0.60
Num timesteps: 1852800
Best mean reward: 0.70 - Last mean reward per episode: 0.67
Num timesteps: 1854000
Best mean reward: 0.70 - Last mean reward per episode: 0.60
Num timesteps: 1855200
Best mean reward: 0.70 - Last mean reward per episode: 0.61
Num timesteps: 1856400
Best mean reward: 0.70 - Last mean reward per episode: 0.59
Num timesteps: 1857600
Best mean reward: 0.70 - Last mean reward per episode: 0.53
Num timesteps: 1858800
Best mean reward: 0.70 - Last mean reward per episode: 0.57
Num timesteps: 1860000
Best mean reward: 0.70 - Last mean reward per episode: 0.58
Num timesteps: 1861200
Best mean reward: 0.70 - Last mean reward per episode: 0.59
Num timesteps: 1862400
Best mean reward: 0.70 - Last mean reward per episode: 0.71
Saving new best model to models/train_stack2/best_model
Num timesteps: 1863600
Best mean reward: 0.71 - Last mean reward per episode: 0.64
Num timesteps: 1864800
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 1866000
Best mean reward: 0.71 - Last mean reward per episode: 0.45
Num timesteps: 1867200
Best mean reward: 0.71 - Last mean reward per episode: 0.65
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 6.99      |
|    ep_rew_mean          | 0.61      |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 76        |
|    time_elapsed         | 597796    |
|    total_timesteps      | 1867776   |
| train/                  |           |
|    approx_kl            | 2.2818577 |
|    clip_fraction        | 0.862     |
|    clip_range           | 0.2       |
|    entropy_loss         | 3.93      |
|    explained_variance   | 0.104     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0232    |
|    n_updates            | 750       |
|    policy_gradient_loss | 0.0488    |
|    std                  | 0.0637    |
|    value_loss           | 0.153     |
---------------------------------------
Num timesteps: 1868400
Best mean reward: 0.71 - Last mean reward per episode: 0.68
Num timesteps: 1869600
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 1870800
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 1872000
Best mean reward: 0.71 - Last mean reward per episode: 0.61
Num timesteps: 1873200
Best mean reward: 0.71 - Last mean reward per episode: 0.64
Num timesteps: 1874400
Best mean reward: 0.71 - Last mean reward per episode: 0.63
Num timesteps: 1875600
Best mean reward: 0.71 - Last mean reward per episode: 0.63
Num timesteps: 1876800
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 1878000
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 1879200
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 1880400
Best mean reward: 0.71 - Last mean reward per episode: 0.65
Num timesteps: 1881600
Best mean reward: 0.71 - Last mean reward per episode: 0.56
Num timesteps: 1882800
Best mean reward: 0.71 - Last mean reward per episode: 0.58
Num timesteps: 1884000
Best mean reward: 0.71 - Last mean reward per episode: 0.66
Num timesteps: 1885200
Best mean reward: 0.71 - Last mean reward per episode: 0.51
Num timesteps: 1886400
Best mean reward: 0.71 - Last mean reward per episode: 0.68
Num timesteps: 1887600
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 1888800
Best mean reward: 0.71 - Last mean reward per episode: 0.52
Num timesteps: 1890000
Best mean reward: 0.71 - Last mean reward per episode: 0.55
Num timesteps: 1891200
Best mean reward: 0.71 - Last mean reward per episode: 0.55
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 7.43      |
|    ep_rew_mean          | 0.538     |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 77        |
|    time_elapsed         | 605701    |
|    total_timesteps      | 1892352   |
| train/                  |           |
|    approx_kl            | 2.5677223 |
|    clip_fraction        | 0.863     |
|    clip_range           | 0.2       |
|    entropy_loss         | 3.97      |
|    explained_variance   | 0.0808    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0137   |
|    n_updates            | 760       |
|    policy_gradient_loss | 0.0486    |
|    std                  | 0.0626    |
|    value_loss           | 0.155     |
---------------------------------------
Num timesteps: 1892400
Best mean reward: 0.71 - Last mean reward per episode: 0.53
Num timesteps: 1893600
Best mean reward: 0.71 - Last mean reward per episode: 0.59
Num timesteps: 1894800
Best mean reward: 0.71 - Last mean reward per episode: 0.51
Num timesteps: 1896000
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 1897200
Best mean reward: 0.71 - Last mean reward per episode: 0.64
Num timesteps: 1898400
Best mean reward: 0.71 - Last mean reward per episode: 0.50
Num timesteps: 1899600
Best mean reward: 0.71 - Last mean reward per episode: 0.64
Num timesteps: 1900800
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 1902000
Best mean reward: 0.71 - Last mean reward per episode: 0.63
Num timesteps: 1903200
Best mean reward: 0.71 - Last mean reward per episode: 0.58
Num timesteps: 1904400
Best mean reward: 0.71 - Last mean reward per episode: 0.59
Num timesteps: 1905600
Best mean reward: 0.71 - Last mean reward per episode: 0.59
Num timesteps: 1906800
Best mean reward: 0.71 - Last mean reward per episode: 0.54
Num timesteps: 1908000
Best mean reward: 0.71 - Last mean reward per episode: 0.52
Num timesteps: 1909200
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 1910400
Best mean reward: 0.71 - Last mean reward per episode: 0.58
Num timesteps: 1911600
Best mean reward: 0.71 - Last mean reward per episode: 0.59
Num timesteps: 1912800
Best mean reward: 0.71 - Last mean reward per episode: 0.55
Num timesteps: 1914000
Best mean reward: 0.71 - Last mean reward per episode: 0.63
Num timesteps: 1915200
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 1916400
Best mean reward: 0.71 - Last mean reward per episode: 0.63
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.66       |
|    ep_rew_mean          | 0.59800005 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 78         |
|    time_elapsed         | 613598     |
|    total_timesteps      | 1916928    |
| train/                  |            |
|    approx_kl            | 2.6042557  |
|    clip_fraction        | 0.866      |
|    clip_range           | 0.2        |
|    entropy_loss         | 4.01       |
|    explained_variance   | 0.0764     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0204     |
|    n_updates            | 770        |
|    policy_gradient_loss | 0.0635     |
|    std                  | 0.0621     |
|    value_loss           | 0.155      |
----------------------------------------
Num timesteps: 1917600
Best mean reward: 0.71 - Last mean reward per episode: 0.61
Num timesteps: 1918800
Best mean reward: 0.71 - Last mean reward per episode: 0.61
Num timesteps: 1920000
Best mean reward: 0.71 - Last mean reward per episode: 0.61
Num timesteps: 1921200
Best mean reward: 0.71 - Last mean reward per episode: 0.67
Num timesteps: 1922400
Best mean reward: 0.71 - Last mean reward per episode: 0.64
Num timesteps: 1923600
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 1924800
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 1926000
Best mean reward: 0.71 - Last mean reward per episode: 0.52
Num timesteps: 1927200
Best mean reward: 0.71 - Last mean reward per episode: 0.59
Num timesteps: 1928400
Best mean reward: 0.71 - Last mean reward per episode: 0.56
Num timesteps: 1929600
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 1930800
Best mean reward: 0.71 - Last mean reward per episode: 0.67
Num timesteps: 1932000
Best mean reward: 0.71 - Last mean reward per episode: 0.66
Num timesteps: 1933200
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 1934400
Best mean reward: 0.71 - Last mean reward per episode: 0.52
Num timesteps: 1935600
Best mean reward: 0.71 - Last mean reward per episode: 0.65
Num timesteps: 1936800
Best mean reward: 0.71 - Last mean reward per episode: 0.63
Num timesteps: 1938000
Best mean reward: 0.71 - Last mean reward per episode: 0.56
Num timesteps: 1939200
Best mean reward: 0.71 - Last mean reward per episode: 0.59
Num timesteps: 1940400
Best mean reward: 0.71 - Last mean reward per episode: 0.58
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 7.45     |
|    ep_rew_mean          | 0.645    |
| time/                   |          |
|    fps                  | 3        |
|    iterations           | 79       |
|    time_elapsed         | 621501   |
|    total_timesteps      | 1941504  |
| train/                  |          |
|    approx_kl            | 2.306545 |
|    clip_fraction        | 0.863    |
|    clip_range           | 0.2      |
|    entropy_loss         | 4.03     |
|    explained_variance   | 0.103    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.00463  |
|    n_updates            | 780      |
|    policy_gradient_loss | 0.0563   |
|    std                  | 0.0615   |
|    value_loss           | 0.154    |
--------------------------------------
Num timesteps: 1941600
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 1942800
Best mean reward: 0.71 - Last mean reward per episode: 0.56
Num timesteps: 1944000
Best mean reward: 0.71 - Last mean reward per episode: 0.61
Num timesteps: 1945200
Best mean reward: 0.71 - Last mean reward per episode: 0.65
Num timesteps: 1946400
Best mean reward: 0.71 - Last mean reward per episode: 0.50
Num timesteps: 1947600
Best mean reward: 0.71 - Last mean reward per episode: 0.63
Num timesteps: 1948800
Best mean reward: 0.71 - Last mean reward per episode: 0.56
Num timesteps: 1950000
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 1951200
Best mean reward: 0.71 - Last mean reward per episode: 0.59
Num timesteps: 1952400
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 1953600
Best mean reward: 0.71 - Last mean reward per episode: 0.65
Num timesteps: 1954800
Best mean reward: 0.71 - Last mean reward per episode: 0.68
Num timesteps: 1956000
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 1957200
Best mean reward: 0.71 - Last mean reward per episode: 0.51
Num timesteps: 1958400
Best mean reward: 0.71 - Last mean reward per episode: 0.51
Num timesteps: 1959600
Best mean reward: 0.71 - Last mean reward per episode: 0.54
Num timesteps: 1960800
Best mean reward: 0.71 - Last mean reward per episode: 0.56
Num timesteps: 1962000
Best mean reward: 0.71 - Last mean reward per episode: 0.58
Num timesteps: 1963200
Best mean reward: 0.71 - Last mean reward per episode: 0.64
Num timesteps: 1964400
Best mean reward: 0.71 - Last mean reward per episode: 0.69
Num timesteps: 1965600
Best mean reward: 0.71 - Last mean reward per episode: 0.57
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 6.92      |
|    ep_rew_mean          | 0.615     |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 80        |
|    time_elapsed         | 629397    |
|    total_timesteps      | 1966080   |
| train/                  |           |
|    approx_kl            | 2.6817913 |
|    clip_fraction        | 0.865     |
|    clip_range           | 0.2       |
|    entropy_loss         | 4.05      |
|    explained_variance   | 0.0816    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00448   |
|    n_updates            | 790       |
|    policy_gradient_loss | 0.0594    |
|    std                  | 0.0612    |
|    value_loss           | 0.154     |
---------------------------------------
Num timesteps: 1966800
Best mean reward: 0.71 - Last mean reward per episode: 0.54
Num timesteps: 1968000
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 1969200
Best mean reward: 0.71 - Last mean reward per episode: 0.59
Num timesteps: 1970400
Best mean reward: 0.71 - Last mean reward per episode: 0.43
Num timesteps: 1971600
Best mean reward: 0.71 - Last mean reward per episode: 0.61
Num timesteps: 1972800
Best mean reward: 0.71 - Last mean reward per episode: 0.61
Num timesteps: 1974000
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 1975200
Best mean reward: 0.71 - Last mean reward per episode: 0.64
Num timesteps: 1976400
Best mean reward: 0.71 - Last mean reward per episode: 0.63
Num timesteps: 1977600
Best mean reward: 0.71 - Last mean reward per episode: 0.61
Num timesteps: 1978800
Best mean reward: 0.71 - Last mean reward per episode: 0.53
Num timesteps: 1980000
Best mean reward: 0.71 - Last mean reward per episode: 0.69
Num timesteps: 1981200
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 1982400
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 1983600
Best mean reward: 0.71 - Last mean reward per episode: 0.58
Num timesteps: 1984800
Best mean reward: 0.71 - Last mean reward per episode: 0.54
Num timesteps: 1986000
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 1987200
Best mean reward: 0.71 - Last mean reward per episode: 0.64
Num timesteps: 1988400
Best mean reward: 0.71 - Last mean reward per episode: 0.55
Num timesteps: 1989600
Best mean reward: 0.71 - Last mean reward per episode: 0.66
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.83       |
|    ep_rew_mean          | 0.57600003 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 81         |
|    time_elapsed         | 637283     |
|    total_timesteps      | 1990656    |
| train/                  |            |
|    approx_kl            | 2.551809   |
|    clip_fraction        | 0.866      |
|    clip_range           | 0.2        |
|    entropy_loss         | 4.07       |
|    explained_variance   | 0.0862     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00504    |
|    n_updates            | 800        |
|    policy_gradient_loss | 0.0659     |
|    std                  | 0.0607     |
|    value_loss           | 0.154      |
----------------------------------------
Num timesteps: 1990800
Best mean reward: 0.71 - Last mean reward per episode: 0.58
Num timesteps: 1992000
Best mean reward: 0.71 - Last mean reward per episode: 0.61
Num timesteps: 1993200
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 1994400
Best mean reward: 0.71 - Last mean reward per episode: 0.63
Num timesteps: 1995600
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 1996800
Best mean reward: 0.71 - Last mean reward per episode: 0.68
Num timesteps: 1998000
Best mean reward: 0.71 - Last mean reward per episode: 0.58
Num timesteps: 1999200
Best mean reward: 0.71 - Last mean reward per episode: 0.52
Num timesteps: 2000400
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 2001600
Best mean reward: 0.71 - Last mean reward per episode: 0.51
Num timesteps: 2002800
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 2004000
Best mean reward: 0.71 - Last mean reward per episode: 0.63
Num timesteps: 2005200
Best mean reward: 0.71 - Last mean reward per episode: 0.58
Num timesteps: 2006400
Best mean reward: 0.71 - Last mean reward per episode: 0.55
Num timesteps: 2007600
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 2008800
Best mean reward: 0.71 - Last mean reward per episode: 0.58
Num timesteps: 2010000
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 2011200
Best mean reward: 0.71 - Last mean reward per episode: 0.63
Num timesteps: 2012400
Best mean reward: 0.71 - Last mean reward per episode: 0.54
Num timesteps: 2013600
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 2014800
Best mean reward: 0.71 - Last mean reward per episode: 0.63
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 8.06       |
|    ep_rew_mean          | 0.62200004 |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 82         |
|    time_elapsed         | 645159     |
|    total_timesteps      | 2015232    |
| train/                  |            |
|    approx_kl            | 2.1837463  |
|    clip_fraction        | 0.851      |
|    clip_range           | 0.2        |
|    entropy_loss         | 4.12       |
|    explained_variance   | 0.0974     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0292    |
|    n_updates            | 810        |
|    policy_gradient_loss | 0.0274     |
|    std                  | 0.0595     |
|    value_loss           | 0.151      |
----------------------------------------
Num timesteps: 2016000
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 2017200
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 2018400
Best mean reward: 0.71 - Last mean reward per episode: 0.64
Num timesteps: 2019600
Best mean reward: 0.71 - Last mean reward per episode: 0.55
Num timesteps: 2020800
Best mean reward: 0.71 - Last mean reward per episode: 0.67
Num timesteps: 2022000
Best mean reward: 0.71 - Last mean reward per episode: 0.59
Num timesteps: 2023200
Best mean reward: 0.71 - Last mean reward per episode: 0.56
Num timesteps: 2024400
Best mean reward: 0.71 - Last mean reward per episode: 0.59
Num timesteps: 2025600
Best mean reward: 0.71 - Last mean reward per episode: 0.64
Num timesteps: 2026800
Best mean reward: 0.71 - Last mean reward per episode: 0.64
Num timesteps: 2028000
Best mean reward: 0.71 - Last mean reward per episode: 0.58
Num timesteps: 2029200
Best mean reward: 0.71 - Last mean reward per episode: 0.58
Num timesteps: 2030400
Best mean reward: 0.71 - Last mean reward per episode: 0.66
Num timesteps: 2031600
Best mean reward: 0.71 - Last mean reward per episode: 0.69
Num timesteps: 2032800
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 2034000
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 2035200
Best mean reward: 0.71 - Last mean reward per episode: 0.55
Num timesteps: 2036400
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 2037600
Best mean reward: 0.71 - Last mean reward per episode: 0.56
Num timesteps: 2038800
Best mean reward: 0.71 - Last mean reward per episode: 0.58
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 7.04     |
|    ep_rew_mean          | 0.616    |
| time/                   |          |
|    fps                  | 3        |
|    iterations           | 83       |
|    time_elapsed         | 653038   |
|    total_timesteps      | 2039808  |
| train/                  |          |
|    approx_kl            | 2.408712 |
|    clip_fraction        | 0.862    |
|    clip_range           | 0.2      |
|    entropy_loss         | 4.17     |
|    explained_variance   | 0.0897   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.00049 |
|    n_updates            | 820      |
|    policy_gradient_loss | 0.0422   |
|    std                  | 0.0587   |
|    value_loss           | 0.154    |
--------------------------------------
Num timesteps: 2040000
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 2041200
Best mean reward: 0.71 - Last mean reward per episode: 0.59
Num timesteps: 2042400
Best mean reward: 0.71 - Last mean reward per episode: 0.66
Num timesteps: 2043600
Best mean reward: 0.71 - Last mean reward per episode: 0.51
Num timesteps: 2044800
Best mean reward: 0.71 - Last mean reward per episode: 0.54
Num timesteps: 2046000
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 2047200
Best mean reward: 0.71 - Last mean reward per episode: 0.56
Num timesteps: 2048400
Best mean reward: 0.71 - Last mean reward per episode: 0.51
Num timesteps: 2049600
Best mean reward: 0.71 - Last mean reward per episode: 0.63
Num timesteps: 2050800
Best mean reward: 0.71 - Last mean reward per episode: 0.50
Num timesteps: 2052000
Best mean reward: 0.71 - Last mean reward per episode: 0.67
Num timesteps: 2053200
Best mean reward: 0.71 - Last mean reward per episode: 0.54
Num timesteps: 2054400
Best mean reward: 0.71 - Last mean reward per episode: 0.68
Num timesteps: 2055600
Best mean reward: 0.71 - Last mean reward per episode: 0.66
Num timesteps: 2056800
Best mean reward: 0.71 - Last mean reward per episode: 0.56
Num timesteps: 2058000
Best mean reward: 0.71 - Last mean reward per episode: 0.52
Num timesteps: 2059200
Best mean reward: 0.71 - Last mean reward per episode: 0.59
Num timesteps: 2060400
Best mean reward: 0.71 - Last mean reward per episode: 0.58
Num timesteps: 2061600
Best mean reward: 0.71 - Last mean reward per episode: 0.52
Num timesteps: 2062800
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 2064000
Best mean reward: 0.71 - Last mean reward per episode: 0.58
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 6.92      |
|    ep_rew_mean          | 0.671     |
| time/                   |           |
|    fps                  | 3         |
|    iterations           | 84        |
|    time_elapsed         | 660909    |
|    total_timesteps      | 2064384   |
| train/                  |           |
|    approx_kl            | 2.6159115 |
|    clip_fraction        | 0.863     |
|    clip_range           | 0.2       |
|    entropy_loss         | 4.21      |
|    explained_variance   | 0.095     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00273   |
|    n_updates            | 830       |
|    policy_gradient_loss | 0.0534    |
|    std                  | 0.0578    |
|    value_loss           | 0.148     |
---------------------------------------
Num timesteps: 2065200
Best mean reward: 0.71 - Last mean reward per episode: 0.59
Num timesteps: 2066400
Best mean reward: 0.71 - Last mean reward per episode: 0.55
Num timesteps: 2067600
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 2068800
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 2070000
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 2071200
Best mean reward: 0.71 - Last mean reward per episode: 0.47
Num timesteps: 2072400
Best mean reward: 0.71 - Last mean reward per episode: 0.61
Num timesteps: 2073600
Best mean reward: 0.71 - Last mean reward per episode: 0.53
Num timesteps: 2074800
Best mean reward: 0.71 - Last mean reward per episode: 0.65
Num timesteps: 2076000
Best mean reward: 0.71 - Last mean reward per episode: 0.61
Num timesteps: 2077200
Best mean reward: 0.71 - Last mean reward per episode: 0.52
Num timesteps: 2078400
Best mean reward: 0.71 - Last mean reward per episode: 0.56
Num timesteps: 2079600
Best mean reward: 0.71 - Last mean reward per episode: 0.45
Num timesteps: 2080800
Best mean reward: 0.71 - Last mean reward per episode: 0.67
Num timesteps: 2082000
Best mean reward: 0.71 - Last mean reward per episode: 0.69
Num timesteps: 2083200
Best mean reward: 0.71 - Last mean reward per episode: 0.58
Num timesteps: 2084400
Best mean reward: 0.71 - Last mean reward per episode: 0.65
Num timesteps: 2085600
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 2086800
Best mean reward: 0.71 - Last mean reward per episode: 0.51
Num timesteps: 2088000
Best mean reward: 0.71 - Last mean reward per episode: 0.65
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 8.05     |
|    ep_rew_mean          | 0.635    |
| time/                   |          |
|    fps                  | 3        |
|    iterations           | 85       |
|    time_elapsed         | 668806   |
|    total_timesteps      | 2088960  |
| train/                  |          |
|    approx_kl            | 2.332383 |
|    clip_fraction        | 0.862    |
|    clip_range           | 0.2      |
|    entropy_loss         | 4.27     |
|    explained_variance   | 0.0934   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0189  |
|    n_updates            | 840      |
|    policy_gradient_loss | 0.0487   |
|    std                  | 0.0568   |
|    value_loss           | 0.153    |
--------------------------------------
Num timesteps: 2089200
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 2090400
Best mean reward: 0.71 - Last mean reward per episode: 0.60
Num timesteps: 2091600
Best mean reward: 0.71 - Last mean reward per episode: 0.66
Num timesteps: 2092800
Best mean reward: 0.71 - Last mean reward per episode: 0.48
Num timesteps: 2094000
Best mean reward: 0.71 - Last mean reward per episode: 0.64
Num timesteps: 2095200
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 2096400
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 2097600
Best mean reward: 0.71 - Last mean reward per episode: 0.65
Num timesteps: 2098800
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 2100000
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 2101200
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 2102400
Best mean reward: 0.71 - Last mean reward per episode: 0.66
Num timesteps: 2103600
Best mean reward: 0.71 - Last mean reward per episode: 0.62
Num timesteps: 2104800
Best mean reward: 0.71 - Last mean reward per episode: 0.68
Num timesteps: 2106000
Best mean reward: 0.71 - Last mean reward per episode: 0.57
Num timesteps: 2107200
Best mean reward: 0.71 - Last mean reward per episode: 0.56
